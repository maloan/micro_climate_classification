{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Main Clustering Script\n",
    "Author: Ananda Kurth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbbc1badb70da5d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 1: Setup  \n",
    "---\n",
    "*This part of the script is to set up the data for the clustering algorithm.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6657e592e8e77f8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1: Import Libraries and Configure Plotting Style\n",
    "*This initial step involves importing essential libraries and setting the plotting style.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "242f9c41f3a7da1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Folium for map visualizations\n",
    "import folium\n",
    "import joblib\n",
    "# Data manipulation and scientific computing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from branca.element import MacroElement\n",
    "from jinja2 import Template\n",
    "# SciPy imports\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import norm, pearsonr\n",
    "# Scikit-learn imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom module imports\n",
    "from API_and_Data.survey_data import SurveyData\n",
    "from msc.MathClass import interpolate_dataframe_to_resolution\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Set Plotting Style\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Setting the aesthetic style of the plots\n",
    "sns.set(style='whitegrid', context='notebook', palette='colorblind', font_scale=1.2)\n",
    "color_palette = sns.color_palette(\"colorblind\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2: Instantiate Data Retrieval and Processing Classes\n",
    "*Here, the instance of the survey Data class is created.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faf0814dd396886a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "survey_data_instance = SurveyData()  # Instance of SurveyData class for handling survey data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be254075fa796d9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3: Configure Data File Paths and Names\n",
    "*Setting up the file paths and names for data files.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57b28151d6cb642c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data File Paths\n",
    "# Grouping all data file path assignments for reference and modification\n",
    "boum_device_list_file = \"../data/boum_device_list.txt\"  # Path to Boum device list file\n",
    "swiss_temperature_file = \"../data/climate-reports-normtables_tre200m0_1991-2020_de.txt\"  # Path to Swiss temperature data file\n",
    "swiss_dni_file = \"../data/climate-reports-normtables_gre000m0_1981-2000_de.txt\"  # Path to Swiss DNI data file\n",
    "survey_data_file = \"../data/survey_data.csv\"  # Path to survey data file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd67864844a47ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Optional Steps: Data Loading\n",
    "#### Loading Boum Data\n",
    "*This section is optional as pickled versions of the resulting datasets will be available later.\n",
    "It covers the steps for loading the Boum data if required.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65199a182921f376"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following code is commented out.\n",
    "# Uncomment to load and process Boum data if needed.\n",
    "\n",
    "# from API_and_Data.boum_API import get_boum_data\n",
    "\n",
    "# boum_data = get_boum_data(boum_device_list_file)\n",
    "# boum_data.to_pickle('../data/boum_data', compression='infer', protocol=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e73fac598f54c51d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Processing\n",
    "*This section is optional as pickled versions of data will be available.\n",
    "It outlines steps for processing data if required.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18eb0731d25593d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Swiss Temperature and DNI Data Analysis\n",
    "*This section focuses on processing and analyzing Swiss data:*\n",
    "- *Data reading using pandas.*\n",
    "- *Timestamp conversion and month-based grouping.*\n",
    "- *Calculation of mean temperatures and DNI and visualization with seaborn.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877d380a0f2e1c07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_mean_dni(monthly_data, title, file_name):\n",
    "    \"\"\"\n",
    "    Plot the average DNI by month.\n",
    "\n",
    "    Args:\n",
    "        monthly_data (pd.Series): Series containing the DNI data.\n",
    "        title (str): Title of the plot.\n",
    "        file_name (str): Name of the file to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(monthly_data.index, monthly_data, color=color_palette[0])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Watt per Square Meter [W/m^2]\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/{file_name}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mean_temperature(monthly_data, title, file_name):\n",
    "    \"\"\"\n",
    "    Plot the average DNI by month.\n",
    "\n",
    "    Args:\n",
    "        monthly_data (pd.Series): Series containing the DNI data.\n",
    "        title (str): Title of the plot.\n",
    "        file_name (str): Name of the file to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(monthly_data.index, monthly_data, color=color_palette[0])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Temperature [Â°C]\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/{file_name}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_swiss_mean_data(data_file, data_type):\n",
    "    \"\"\"\n",
    "    Process the Swiss mean data (temperature or DNI) from the specified file.\n",
    "\n",
    "    Args:\n",
    "        data_file (str): The file path for the data.\n",
    "        data_type (str): Type of the data ('Temperature' or 'DNI').\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series of median values (temperature or DNI).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        southern_stations = ['Cimetta', 'Locarno', 'Lugano', 'Magadino', 'Poschiavo', 'Stabio']\n",
    "        full_month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September',\n",
    "                            'October', 'November', 'December']\n",
    "\n",
    "        df = pd.read_csv(data_file, sep=\"\\t\")  # Assuming tab-separated file\n",
    "\n",
    "        # Filter based on stations and altitude\n",
    "        to_drop = df[(~df['Station'].isin(southern_stations)) & (df['Stationshoehe'] >= 1600)].index\n",
    "        df.drop(to_drop, inplace=True)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        df.drop(columns=['Station', 'Stationshoehe', 'CH Koordinaten', 'Referenzperiode', 'Jahr'], inplace=True)\n",
    "\n",
    "        df.columns = full_month_names  # Renaming columns to full month names\n",
    "\n",
    "        monthly_median = df.median()  # Calculating medians           \n",
    "        data_type_str = 'temp' if data_type == 'Temperature' else 'dni'\n",
    "        monthly_median.to_pickle(f\"../data/{data_type_str}_value_CH\", compression=\"infer\", protocol=5)\n",
    "\n",
    "        plot_type = plot_mean_temperature if data_type == 'Temperature' else plot_mean_dni\n",
    "        title = f\"{data_type} per Month\\nNormals\"\n",
    "        file_name = f\"climate_reports_by_month_{data_type_str}.png\"\n",
    "        plot_type(monthly_median, title, file_name)\n",
    "\n",
    "        return monthly_median\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {data_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Usage for temperature data\n",
    "temp_value_CH = process_swiss_mean_data(swiss_temperature_file, 'Temperature')\n",
    "\n",
    "# Usage for DNI data\n",
    "dni_value_CH = process_swiss_mean_data(swiss_dni_file, 'DNI')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a39a8d2ec7515f42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1.4 Loading Pickled Data Sets\n",
    "*This part covers the loading of previously pickled data sets,\n",
    "which are essential for subsequent data processing and analysis steps.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d005c4e9977e26f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize dataframes\n",
    "boum_data = pd.DataFrame()\n",
    "weather_data = pd.DataFrame()\n",
    "history_data = pd.DataFrame()\n",
    "survey_data = pd.DataFrame()\n",
    "historical_weather_data = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cbdaf89683af509"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    # Loading various datasets required for the analysis\n",
    "    # BOUM data containing various sensor measurements\n",
    "    boum_data = pd.read_pickle(\"../data/boum_data_new\")\n",
    "\n",
    "    # Survey data with survey information for survey data analysis\n",
    "    survey_data = pd.read_csv(\"../data/survey_data.csv\")\n",
    "\n",
    "    # Weather data corresponding to the BOUM device data for each location\n",
    "    weather_data = pd.read_pickle(\"../data/weather_data_call\")\n",
    "\n",
    "    # Swiss mean Direct Normal Irradiance (DNI) values\n",
    "    dni_value_CH = pd.read_pickle(\"../data/dni_value_CH\")\n",
    "\n",
    "    # Swiss mean temperature values\n",
    "    temp_value_CH = pd.read_pickle(\"../data/temp_value_CH\")\n",
    "\n",
    "    # Historical weather data for each month \n",
    "    historical_weather_data = pd.read_pickle(\"../data/hist_weather_pickled\")\n",
    "\n",
    "except FileNotFoundError as file_not_found_error:\n",
    "    print(f\"Error loading file: {file_not_found_error}\")\n",
    "except Exception as exception:\n",
    "    print(f\"An unexpected error occurred: {exception}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be6b0f38685eed77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 2: Data Cleaning and Preprocessing\n",
    "---\n",
    "*In this section, data cleaning and preprocessing steps are outlined,\n",
    "including the extraction of coordinates from survey data files and preprocessing of timestamps in data files\n",
    "to ensure consistency and accuracy.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea46e4107528ede"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1: Extracting Coordinates from Survey Data\n",
    "*The procedure detailed here extracts coordinates of each BOUM device from the survey data file,\n",
    "involving reading device lists, filtering survey data,\n",
    "and creating a coordinate dictionary.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b148bb3cfbb8d32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_coordinates(device_list_file, survey_file):\n",
    "    \"\"\"\n",
    "    Extracts coordinates from survey data for devices listed in the device list file.\n",
    "\n",
    "    Args:\n",
    "        device_list_file (str): Filename containing the list of device IDs.\n",
    "        survey_file (str): Filename of the survey data CSV file.\n",
    "    Returns:\n",
    "        dict: Dictionary mapping device IDs to their corresponding latitude and longitude.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"../data/{device_list_file}\"):\n",
    "        print(f\"Device list file not found: {device_list_file}\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        survey_file = pd.read_csv(survey_file)\n",
    "        survey_file = survey_file.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "\n",
    "        with open(f\"../data/{device_list_file}\", \"r\") as file:\n",
    "            device_list = {line.strip() for line in file}  # Using a set for efficient lookup\n",
    "\n",
    "        # Filtering the survey data for device IDs present in the device list\n",
    "        filtered_data = survey_file[survey_file[\"deviceId_boum\"].isin(device_list)]\n",
    "\n",
    "        return {ro[\"deviceId_boum\"]: (ro[\"latitude\"], ro[\"longitude\"]) for index, ro in\n",
    "                filtered_data.iterrows()}\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Survey data file not found: {survey_data_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "coordinates_dict = get_coordinates(boum_device_list_file, survey_data_file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995ba2a54833fff3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2: Preprocessing Timestamps in Data Files\n",
    "*This process involves converting 'timestamp' columns to datetime objects,\n",
    "normalizing timestamps, and interpolating data to a regular interval.\n",
    "The timestamp column is then set as the DataFrame index.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5cdc88955d4055"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_timestamps(data_frame):\n",
    "    \"\"\"\n",
    "    Preprocesses the timestamps in the given DataFrame.\n",
    "\n",
    "    Converts the timestamp to a datetime object, normalizes it,\n",
    "    interpolates the data to a specified resolution, and sets the timestamp as the index.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing a 'timestamp' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with preprocessed timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Converting 'timestamp' to datetime and removing timezone info\n",
    "        data_frame[\"timestamp\"] = pd.to_datetime(data_frame[\"timestamp\"]).dt.tz_localize(None)\n",
    "\n",
    "        # Converting timestamp to seconds (from milliseconds)\n",
    "        data_frame[\"timestamp\"] = data_frame[\"timestamp\"].values.astype(np.int64) // 10 ** 6 // 1000\n",
    "\n",
    "        # Interpolating data to a regular interval (600 seconds)\n",
    "        data_frame = interpolate_dataframe_to_resolution(data_frame, \"timestamp\", 600, data_frame.columns, \"values\")\n",
    "\n",
    "        # Converting back to datetime and setting as index\n",
    "        data_frame[\"timestamp\"] = pd.to_datetime(data_frame[\"timestamp\"], unit=\"s\")\n",
    "        data_frame.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "        return data_frame\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in preprocess_timestamps: {e}\")\n",
    "        return data_frame  # Returning the original DataFrame in case of an error\n",
    "\n",
    "\n",
    "# Applying the preprocessing function to the datasets\n",
    "boum_data = preprocess_timestamps(boum_data)\n",
    "weather_data = preprocess_timestamps(weather_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0109be8ea0384c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Dropping columns that contain 'time' in their names from weather_data*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96fd68f5f4b9c0fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is done to remove any redundant or unnecessary time-related information\n",
    "time_columns = weather_data.filter(like='time').columns\n",
    "if len(time_columns) > 0:\n",
    "    weather_data.drop(columns=time_columns, inplace=True)\n",
    "else:\n",
    "    print(\"No columns to drop that contain 'time'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e722ec45fcbd2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3: Extracting Monthly Mean Temperatures and Radiation\n",
    "*This section includes the extraction and summarization of monthly mean temperatures and radiation from historical weather data,\n",
    "including calculating mean values, grouping by month, and generating descriptive statistics.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b104501ec905d48e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_historical_monthly_mean(data):\n",
    "    \"\"\"\n",
    "    Extracts and summarizes monthly mean temperatures and radiation from historical weather data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing historical weather data with a 'timestamp' index.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataFrames containing monthly descriptive statistics for temperature and radiation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(data.index):\n",
    "            data.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "        # Identifying temperature and radiation columns\n",
    "        temp_cols = [col for col in data if \"temperature_2m\" in col]\n",
    "        radiation_columns = [col for col in data if \"direct_normal\" in col]\n",
    "\n",
    "        # Calculating monthly means and descriptive statistics\n",
    "        monthly_temp = data[temp_cols].mean(axis=1).groupby(data.index.month).describe()\n",
    "        monthly_rad = data[radiation_columns].mean(axis=1).groupby(data.index.month).describe()\n",
    "\n",
    "        return monthly_temp, monthly_rad\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in extract_historical_monthly_mean: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Applying the function to the historical weather data\n",
    "monthly_temperature, monthly_radiation = extract_historical_monthly_mean(historical_weather_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "134390f53a354a15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4: Calculating Percentiles for Temperature and Radiation Data\n",
    "*Thresholds for temperature and radiation data are calculated based on percentiles.\n",
    "This involves defining default percentiles, validating input data,\n",
    "and calculating thresholds for each month using statistical methods.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2988af816bd313b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_thresholds(data, month_data, mode, percentiles=None):\n",
    "    \"\"\"\n",
    "    Calculates thresholds for the DNI and temperature data based on percentiles.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Series containing mean values for each month, indexed by month number (0-11).\n",
    "        month_data (pd.DataFrame): DataFrame containing monthly statistics, indexed by month number (1-12).\n",
    "        mode (str): The type of data ('Temperature' for temperature, 'Radiation' for radiation).\n",
    "        percentiles (list): List of percentiles to calculate thresholds.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists containing calculated thresholds for each month.\n",
    "    \"\"\"\n",
    "\n",
    "    month_data.reset_index(inplace=True)  # Resetting the index of the monthly data to 0-11\n",
    "\n",
    "    if percentiles is None:\n",
    "        percentiles = [20, 50, 80]\n",
    "\n",
    "    if not (0 <= min(percentiles) <= 100 and 0 <= max(percentiles) <= 100):  # Checking if the percentiles are valid\n",
    "        raise ValueError(\"Percentiles must be between 0 and 100\")\n",
    "\n",
    "    if len(data) != 12 or len(month_data) != 12:  # Checking if the data and month_data have the same number of months\n",
    "        raise ValueError(\"Data and month_data should each contain 12 months\")\n",
    "\n",
    "    thresholds = []\n",
    "    for month_num in range(0, 12):  # Calculating thresholds for each month\n",
    "        mean = data[month_num]  # Getting the mean value for the month\n",
    "        std = month_data.loc[month_num, \"std\"]  # Getting the standard deviation for the month\n",
    "        month_thresholds = [max(0, int(norm.ppf(p / 100, loc=mean, scale=std))) if mode == 'Radiation' else int(\n",
    "            norm.ppf(p / 100, loc=mean, scale=std)) for p in percentiles]  # Calculating thresholds for each percentile\n",
    "        thresholds.append(month_thresholds)  # Adding the calculated thresholds to the list\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "# Applying the function to calculate thresholds\n",
    "temperature_thresholds = calculate_thresholds(temp_value_CH, monthly_temperature, 'Temperature')\n",
    "radiation_thresholds = calculate_thresholds(dni_value_CH, monthly_radiation, 'Radiation')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d5ad05895408762"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5: Correcting Temperature and Voltage Readings in Data\n",
    "*The focus here is on correcting temperature and voltage readings in a DataFrame.\n",
    "Steps include creating a DataFrame copy,\n",
    "filtering and correcting temperature and voltage data, resampling to an hourly frequency,\n",
    "and plotting for comparison.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5632c214513f4186"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def correct_data(data_frame, temp_correction_factor=0.775, temp_offset=2.748, voltage_threshold=5.0):\n",
    "    \"\"\"\n",
    "    Corrects the temperature and voltage readings in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing the data to be corrected.\n",
    "        temp_correction_factor (float): Multiplicative factor for temperature correction.\n",
    "        temp_offset (float): Offset value to be added to temperature after applying a correction factor.\n",
    "        voltage_threshold (float): Threshold value for filtering voltage readings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Corrected DataFrame.\n",
    "    \"\"\"\n",
    "    # Creating a copy of the DataFrame to avoid modifying the original data\n",
    "    df = data_frame.copy()\n",
    "\n",
    "    # Setting 'timestamp' as index if it exists\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    # Identifying temperature and voltage columns\n",
    "    temp_cols = [col for col in df.columns if col.startswith(\"temperature_boum\")]\n",
    "    volt_cols = [col.replace(\"temperature\", \"solarVoltage\") for col in temp_cols]\n",
    "\n",
    "    # Converting temperature and voltage columns to numeric, coerce errors\n",
    "    df = df[temp_cols + volt_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Creating a copy of the DataFrame for comparison later\n",
    "    # df_original = df.copy()\n",
    "\n",
    "    # Filtering out voltage values above the threshold\n",
    "    df[volt_cols] = df[volt_cols][df[volt_cols] <= voltage_threshold]\n",
    "\n",
    "    # Applying the temperature correction factor and offset\n",
    "    df[temp_cols] = temp_correction_factor * df[temp_cols] + temp_offset\n",
    "\n",
    "    # Removing consecutive duplicate readings\n",
    "    df[temp_cols + volt_cols] = df[temp_cols + volt_cols].mask(\n",
    "        df[temp_cols + volt_cols].diff().round(2) == 0)\n",
    "\n",
    "    # Resampling data hourly and dropping rows with all NaN values\n",
    "    df_corrected = df.resample(\"30T\").mean().dropna(how=\"all\")\n",
    "\n",
    "    # Plotting temperature comparison before and after correction if that is wanted\n",
    "    # plot_temperature_comparison(df_corrected, df_original, temperature_columns)\n",
    "\n",
    "    return df_corrected\n",
    "\n",
    "\n",
    "def plot_temperature_comparison(df_corrected, df_original, temperature_cols):\n",
    "    \"\"\"\n",
    "    Plots a comparison of original and corrected temperature readings.\n",
    "\n",
    "    Args:\n",
    "        df_corrected (pd.DataFrame): DataFrame with corrected data.\n",
    "        df_original (pd.DataFrame): DataFrame with original data.\n",
    "        temperature_cols (list): List of column names for temperature data.\n",
    "    \"\"\"\n",
    "    # Setting up the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plotting each temperature column from original and corrected data\n",
    "    for col in temperature_cols:\n",
    "        plt.plot(df_original.index, df_original[col], color=color_palette[0], alpha=0.5, label='Original')\n",
    "        plt.plot(df_corrected.index, df_corrected[col], color=color_palette[1], alpha=1, label='Corrected')\n",
    "\n",
    "    # Setting the x-axis limits\n",
    "    plt.xlim(pd.Timestamp('2023-06-01'), df_original.index.max())\n",
    "\n",
    "    # Adding plot title and axis labels\n",
    "    plt.title('Temperature Readings: Original vs Corrected')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Temperature [Â°C]')\n",
    "\n",
    "    # Creating and adding a legend to the plot\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc=\"upper right\")\n",
    "\n",
    "    # Adding grid, adjusting layout, and saving the plot as a PNG file\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../plots/Temperature_Comparison_Plot.png')\n",
    "\n",
    "    # Displaying the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Applying the correction function to boum_data\n",
    "boum_data = correct_data(boum_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "561919b1024e90a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identifying temperature columns in the boum_data DataFrame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d6705a9205c4f51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This includes all columns that contain the string 'temperature_boum'\n",
    "temperature_columns = boum_data.filter(like='temperature_boum').columns.tolist()\n",
    "\n",
    "# Creating a list of corresponding voltage columns,\n",
    "# this is done by replacing 'temperature' with 'solarVoltage' in each temperature column name\n",
    "voltage_columns = [col.replace(\"temperature\", \"solarVoltage\") for col in temperature_columns]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc0a78afb309dd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6: Identifying Time Range with Highest Mean Differences in Temperature and Voltage\n",
    "*This section focuses on identifying the time range with the highest combined mean differences in temperature and voltage between locations.\n",
    "It involves calculating mean hourly differences, normalizing these values, and pinpointing the time range with the top 15% differences.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a61f518a50d33d96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_peak_time():\n",
    "    \"\"\"\n",
    "    Identifies the time range with the highest combined mean differences in temperature and voltage.\n",
    "\n",
    "    This function calculates the mean hourly differences for both temperature and voltage, \n",
    "    normalizes these differences, and then finds the time range where these combined differences are in the top 10%.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the minimum and maximum hours of the identified peak time range.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating the absolute difference between consecutive hourly readings for temperature and voltage\n",
    "    hourly_temp_diff = boum_data[temperature_columns].diff(axis=1).abs()\n",
    "    hourly_volt_diff = boum_data[voltage_columns].diff(axis=1).abs()\n",
    "\n",
    "    # Grouping by hour and computing the mean difference for each hour\n",
    "    mean_hourly_temp_diff = hourly_temp_diff.groupby(hourly_temp_diff.index.hour).mean()\n",
    "    mean_hourly_volt_diff = hourly_volt_diff.groupby(hourly_volt_diff.index.hour).mean()\n",
    "\n",
    "    # Normalizing the differences by their respective max values and averaging across columns\n",
    "    mean_temp_diff = (mean_hourly_temp_diff / mean_hourly_temp_diff.max()).mean(axis=1)\n",
    "    mean_volt_diff = (mean_hourly_volt_diff / mean_hourly_volt_diff.max()).mean(axis=1)\n",
    "\n",
    "    # Combining the normalized temperature and voltage differences\n",
    "    common_line = (mean_temp_diff + mean_volt_diff) / 2\n",
    "\n",
    "    # Identifying the top percentage of hours with the highest mean differences\n",
    "    top_percentage = 0.15\n",
    "    threshold_value = common_line.quantile(1 - top_percentage)\n",
    "    top_percent_index = common_line[common_line >= threshold_value].index\n",
    "\n",
    "    # Determining the range of hours in the top percentage\n",
    "    minimum_time = min(top_percent_index)\n",
    "    maximum_time = max(top_percent_index)\n",
    "    print(\n",
    "        f\"Time range with the highest combined mean differences (top {top_percentage * 100}%): {minimum_time} to {maximum_time} hours\")\n",
    "\n",
    "    # Plotting the peak hours\n",
    "    plot_peak_hours(common_line, mean_temp_diff, mean_volt_diff, top_percent_index)\n",
    "\n",
    "    return minimum_time, maximum_time\n",
    "\n",
    "\n",
    "def plot_peak_hours(common_line, mean_temp_diff, mean_volt_diff, top_percent_index):\n",
    "    \"\"\"\n",
    "    Plots the hourly normalized differences in temperature and voltage, highlighting the peak hours.\n",
    "\n",
    "    Args:\n",
    "        common_line (pd.Series): A series containing the combined normalized differences of temperature and voltage.\n",
    "        mean_temp_diff (pd.Series): A series containing the mean hourly normalized temperature differences.\n",
    "        mean_volt_diff (pd.Series): A series containing the mean hourly normalized voltage differences.\n",
    "        top_percent_index (pd.Index): An index of hours that are in the top percentage of combined differences.\n",
    "\n",
    "    This function creates a plot to visually compare the original and corrected temperature readings.\n",
    "    \"\"\"\n",
    "    # Setting up the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Defining colors for the plot\n",
    "    color_temp = color_palette[0]\n",
    "    color_volt = color_palette[1]\n",
    "    color_combined = color_palette[2]\n",
    "\n",
    "    # Plotting the normalized differences\n",
    "    ax1.set_xlabel(\"Hour of Day\", color='black')\n",
    "    ax1.set_ylabel(\"Normalized Difference\", color='black')\n",
    "    ax1.plot(common_line.index, common_line, color=color_combined, label=\"Combined Difference\")\n",
    "    ax1.fill_between(top_percent_index, 0, common_line.loc[top_percent_index], color=color_combined, alpha=0.5)\n",
    "    ax1.plot(mean_temp_diff.index, mean_temp_diff, color=color_temp, linestyle=\"--\", label=\"Temperature\")\n",
    "    ax1.plot(mean_volt_diff.index, mean_volt_diff, color=color_volt, linestyle=\"--\", label=\"Voltage\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor='black')\n",
    "\n",
    "    # Adding a title and customizing ticks\n",
    "    ax1.set_title(\"Mean Hourly Normalized Differences\")\n",
    "    ax1.set_xticks(range(0, 24))\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Finalizing and displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/Hourly_Normalized_Differences.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Using the function to find the peak time based on temperature and voltage data\n",
    "min_time, max_time = find_peak_time()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502a12bf5b9eb81c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Adding a new column to the boum_data DataFrame and the weather_data DataFrame, to more easily select specific months.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede910c6ea5f52a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding a new column 'm' to boum_data representing the month number as an integer\n",
    "boum_data[\"m\"] = boum_data.index.month.astype(int)\n",
    "\n",
    "# Adding another column 'month' to boum_data with the month and day formatted as \"mm-dd\"\n",
    "boum_data[\"month\"] = boum_data.index.strftime(\"%m-%d\")\n",
    "\n",
    "# Resetting the index of boum_data to convert 'timestamp' from the index to a regular column\n",
    "boum_data.reset_index(inplace=True)\n",
    "\n",
    "# For weather_data, adding a new column 'month' representing the month number as an integer\n",
    "weather_data.loc[:, \"month\"] = weather_data.index.month.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4982d6df4c1734fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 3: Data Processing and Preparation for Clustering\n",
    "---\n",
    "*This part of the notebook details the steps involved in processing and preparing data for clustering analysis,\n",
    "including preprocessing, merging temperature and radiation data,\n",
    "and refining the preprocessed data for easier analysis.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c4c2cc619e2eb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1: Preprocessing Data for Clustering\n",
    "*This function, `preprocess_data_for_clustering`, prepares data for clustering by processing temperature,\n",
    "solar voltage, and radiation data for specified months.\n",
    "It involves creating pivot tables for both Boum and weather data, and appending these tables to a compiled DataFrame.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4faaa70c0d232b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_column_names(boum_id):\n",
    "    \"\"\"\n",
    "    Generates column names for temperature and solar voltage data based on a given BOUM ID.\n",
    "\n",
    "    Args:\n",
    "        boum_id (str): The identifier for a specific BOUM device.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the formatted column names for temperature and solar voltage data.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"temperature_2m_{boum_id}\",\n",
    "        f\"temperature_boum_{boum_id}_boum_{boum_id}\",\n",
    "        f\"solarVoltage_boum_{boum_id}_boum_{boum_id}\",\n",
    "        f\"direct_normal_irradiance_{boum_id}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_daily_mean(target_month, radiation_column, temperature_column, weather_df):\n",
    "    \"\"\"\n",
    "    Samples daily mean values for specified temperature and radiation columns in a given month.\n",
    "\n",
    "    Args:\n",
    "        target_month (int): The month number to filter the data.\n",
    "        radiation_column (str): The name of the radiation column.\n",
    "        temperature_column (str): The name of the temperature column.\n",
    "        weather_df (pd.DataFrame): The DataFrame containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataFrames containing daily max values for radiation and temperature.\n",
    "    \"\"\"\n",
    "    seasonal_data = weather_df[weather_df[\"month\"] == target_month].copy()\n",
    "    seasonal_data.drop(columns=[col for col in seasonal_data.columns if \"time\" in col], inplace=True)\n",
    "\n",
    "    temperature_data = seasonal_data[temperature_column].resample(\"D\").mean()\n",
    "    radiation_data = seasonal_data[radiation_column].resample(\"D\").mean()\n",
    "    temperature_categories = np.digitize(temperature_data, temperature_thresholds[target_month - 1], right=True)\n",
    "    radiation_categories = np.digitize(radiation_data, radiation_thresholds[target_month - 1], right=True)\n",
    "    temperature_data = pd.DataFrame(temperature_data)\n",
    "    radiation_data = pd.DataFrame(radiation_data)\n",
    "    temperature_data[\"temperature_category\"] = temperature_categories\n",
    "    radiation_data[\"radiation_category\"] = radiation_categories\n",
    "    temperature_data[\"month\"] = temperature_data.index.month\n",
    "    radiation_data[\"month\"] = radiation_data.index.month\n",
    "    return radiation_data, temperature_data\n",
    "\n",
    "\n",
    "def extract_data(boum_df, value_columns, data_value, target_month):\n",
    "    \"\"\"\n",
    "    Extracts and processes data for a specific month, filtering it based on certain time constraints.\n",
    "\n",
    "    Args:\n",
    "        boum_df (pd.DataFrame): The DataFrame containing BOUM data.\n",
    "        value_columns (str): The name of the column to be extracted.\n",
    "        data_value (pd.DataFrame): DataFrame containing additional data to merge.\n",
    "        target_month (int): The target month for data extraction.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with extracted and merged data.\n",
    "    \"\"\"\n",
    "    extracted_data = boum_df[[value_columns, \"timestamp\"]].copy()\n",
    "    try:\n",
    "        extracted_data[\"hour\"] = pd.to_datetime(extracted_data[\"timestamp\"]).dt.hour\n",
    "        extracted_data[\"month\"] = pd.to_datetime(extracted_data[\"timestamp\"]).dt.month\n",
    "        extracted_data = extracted_data[extracted_data[\"month\"] == target_month]\n",
    "        extracted_data = extracted_data[extracted_data[\"hour\"].between(min_time, max_time)].set_index(\n",
    "            \"timestamp\").resample(\"D\").median()\n",
    "        extracted_data = pd.merge_ordered(extracted_data, data_value, on=[\"timestamp\", \"month\"], how=\"outer\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Error in extract_data: {ex}\")\n",
    "        return pd.DataFrame()\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def create_pivot_table(data, category_column, value_column):\n",
    "    \"\"\"\n",
    "    Helper function to create pivot tables.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): DataFrame containing the data.\n",
    "    category_column (str): Column name to group by.\n",
    "    value_column (str): Column name for calculating mean.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Pivot table based on the specified category and value columns.\n",
    "    \"\"\"\n",
    "    return data.groupby(category_column)[value_column].mean().reset_index().pivot_table(\n",
    "        index=category_column, values=value_column, aggfunc=\"mean\")\n",
    "\n",
    "\n",
    "def append_to_final(final_data, solar_pivot, temperature_pivot, radiation_pivot, temp_pivot):\n",
    "    \"\"\"\n",
    "    Appends given pivot tables to the final DataFrame.\n",
    "\n",
    "    Args:\n",
    "        final_data (pd.DataFrame): The DataFrame to append to.\n",
    "        solar_pivot (pd.DataFrame): Pivot table for solar data.\n",
    "        temperature_pivot (pd.DataFrame): Pivot table for temperature data.\n",
    "        radiation_pivot (pd.DataFrame): Pivot table for radiation data.\n",
    "        temp_pivot (pd.DataFrame): Additional pivot table for temperature data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated final DataFrame with appended data.\n",
    "    \"\"\"\n",
    "    if final_data.empty:\n",
    "        # If final_data is empty, initialize it with the joined pivot tables\n",
    "        final_data = temp_pivot.join(radiation_pivot).join(temperature_pivot).join(solar_pivot)\n",
    "    else:\n",
    "        # If final_data already contains data, append the new pivot tables\n",
    "        final_data = final_data.join(temp_pivot).join(radiation_pivot).join(temperature_pivot).join(solar_pivot)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def preprocess_data_for_clustering(boum_df, weather_df, coord_dict, month):\n",
    "    \"\"\"\n",
    "    Preprocesses data for clustering by extracting, processing, and merging temperature and radiation data.\n",
    "\n",
    "    Args:\n",
    "        boum_df (pd.DataFrame): DataFrame containing BOUM data.\n",
    "        weather_df (pd.DataFrame): DataFrame containing weather data.\n",
    "        coord_dict (dict): Dictionary mapping BOUM IDs to their coordinates.\n",
    "        month (int): Month number to filter and process the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame ready for clustering analysis.\n",
    "    \"\"\"\n",
    "    avg_temp_and_radiation_by_category_and_month = pd.DataFrame()\n",
    "    for boum_id, coordinates in coord_dict.items():\n",
    "        boum_id = boum_id[:8]\n",
    "        try:\n",
    "            temperature_column, boum_temperature_column, boum_solar_column, radiation_column = get_column_names(boum_id)\n",
    "            weather_radiation_data, weather_temperature_data = sample_daily_mean(month, radiation_column,\n",
    "                                                                                 temperature_column,\n",
    "                                                                                 weather_df)\n",
    "            boum_temperature_data = extract_data(boum_df, boum_temperature_column, weather_temperature_data, month)\n",
    "            boum_solar_data = extract_data(boum_df, boum_solar_column, weather_radiation_data, month)\n",
    "\n",
    "            pivot_boum_solar = create_pivot_table(boum_solar_data, ['radiation_category'], boum_solar_column)\n",
    "            pivot_boum_temperature = create_pivot_table(boum_temperature_data, [\"temperature_category\"],\n",
    "                                                        boum_temperature_column)\n",
    "            pivot_radiation = create_pivot_table(weather_radiation_data, [\"radiation_category\", \"month\"],\n",
    "                                                 radiation_column)\n",
    "            pivot_temperature = create_pivot_table(weather_temperature_data, [\"temperature_category\", \"month\"],\n",
    "                                                   temperature_column)\n",
    "            avg_temp_and_radiation_by_category_and_month = append_to_final(avg_temp_and_radiation_by_category_and_month,\n",
    "                                                                           pivot_boum_solar,\n",
    "                                                                           pivot_boum_temperature, pivot_radiation,\n",
    "                                                                           pivot_temperature)\n",
    "            gc.collect()\n",
    "        except KeyError as key_error:\n",
    "            print(f\"Error in preprocess_data_for_clustering: {key_error}\")\n",
    "            continue\n",
    "        except Exception as exc:\n",
    "            print(f\"An unexpected error occurred for BOUM ID {boum_id}: {exc}\")\n",
    "            continue\n",
    "    return avg_temp_and_radiation_by_category_and_month"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a28df1b376fcdec8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1.1: Data Preprocessing Pipeline for Specific Months\n",
    "*For each month, the `preprocess_data_for_clustering` function is executed, processing data and returning a DataFrame.\n",
    "These DataFrames are then concatenated, forming the dataset for further analysis.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfbd75856120ade9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenating the preprocessed data for each month from April to October into a single DataFrame.\n",
    "resulting_data = pd.concat(\n",
    "    [preprocess_data_for_clustering(boum_data, weather_data, coordinates_dict, month) for month in range(4, 11)]\n",
    ")\n",
    "\n",
    "# Displaying the resulting concatenated DataFrame\n",
    "resulting_data.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e20db47ee48222c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2: Cleanup of Preprocessed Data for Analysis\n",
    "*This code streamlines the DataFrame containing preprocessed data by dropping certain rows.\n",
    "The focus is on retaining the most significant values for temperature and radiation for in-depth analysis.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8118b7563dcf674f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def refine_resulting_data(data):\n",
    "    \"\"\"\n",
    "    Refines the provided DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to be refined.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dataframe containing the processed data.\n",
    "    \"\"\"\n",
    "    # Dropping columns related to direct normal irradiance and temperature\n",
    "    res_data = data.drop(\n",
    "        [col for col in data.columns if \"direct_normal_irradiance\" in col or \"temperature_2m\" in col], axis=1\n",
    "    ).reset_index()\n",
    "    return res_data\n",
    "\n",
    "\n",
    "# Applying the function to refine the resulting data\n",
    "resulting_data = refine_resulting_data(resulting_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4422dd4a73747c5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4: Melting and Transforming Data for Analysis\n",
    "*This section describes the process of melting and transforming a DataFrame according to a specified mode (either temperature or dni).\n",
    "The approach involves two main stages: preparation, which includes data setup and column selection, and melting, using pandas to reshape the data into a long format for detailed analysis.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa9e6ba4910a63b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def melt_data(data_frame, mode):\n",
    "    \"\"\"\n",
    "    Melts and transforms the given DataFrame based on the specified mode (temperature or radiation).\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): The DataFrame to be melted.\n",
    "        mode (str): The mode for melting, either 'Temperature' or 'Radiation'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the pivot table and the melted DataFrame.\n",
    "    \"\"\"\n",
    "    # Prepare the data based on the specified mode\n",
    "    data_frame, value_vars = prepare_data(data_frame, mode)\n",
    "\n",
    "    # Set the ID variables based on the mode\n",
    "    id_vars = \"radiation_category\" if mode == \"Radiation\" else \"temperature_category\"\n",
    "\n",
    "    # Set the value name based on the mode\n",
    "    value_name = \"solar_voltage\" if mode == \"Radiation\" else \"temperature\"\n",
    "\n",
    "    # Melt the DataFrame\n",
    "    merged_df = pd.melt(\n",
    "        data_frame,\n",
    "        id_vars=[id_vars],\n",
    "        value_vars=value_vars,\n",
    "        var_name=\"sensor\",\n",
    "        value_name=value_name,\n",
    "    )\n",
    "\n",
    "    # Create a pivot table from the melted DataFrame\n",
    "    pivot_table = create_melted_pivot_table(merged_df, id_vars, value_name)\n",
    "\n",
    "    return pivot_table, merged_df\n",
    "\n",
    "\n",
    "def create_melted_pivot_table(merged_df, id_vars, value_name):\n",
    "    \"\"\"\n",
    "    Creates a pivot table from the melted DataFrame.\n",
    "\n",
    "    Args:\n",
    "        merged_df (pd.DataFrame): The melted DataFrame.\n",
    "        id_vars (str): The identifier variables for pivoting.\n",
    "        value_name (str): The name of the value column in the pivot table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pivot table based on the specified criteria.\n",
    "    \"\"\"\n",
    "    # Creating the pivot table\n",
    "    pivot_table = merged_df.pivot_table(\n",
    "        index=\"sensor\", columns=[id_vars], values=value_name, aggfunc=[\"mean\"]\n",
    "    )\n",
    "\n",
    "    # Cleaning up the pivot table\n",
    "    pivot_table.columns = pivot_table.columns.droplevel(0)\n",
    "    pivot_table.fillna(0, inplace=True)\n",
    "\n",
    "    return pivot_table\n",
    "\n",
    "\n",
    "def prepare_data(data_frame, mode):\n",
    "    \"\"\"\n",
    "    Prepares the data by dropping irrelevant columns based on the specified mode.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): The original DataFrame.\n",
    "        mode (str): The mode for data preparation, either 'Temperature' or 'Radiation'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the prepared DataFrame and the value variables.\n",
    "    \"\"\"\n",
    "    # Dropping irrelevant columns based on mode and extracting value variables\n",
    "    if mode == \"Radiation\":\n",
    "        data_frame = data_frame.drop(columns=[col for col in data_frame.columns if\n",
    "                                              \"temperature_boum\" in col or \"temperature_category\" in col or \"temperature_2m\" in col]\n",
    "                                     )\n",
    "        value_vars = data_frame.columns.drop([\"month\", \"radiation_category\"])\n",
    "    else:\n",
    "        data_frame = data_frame.drop(columns=[col for col in data_frame.columns if\n",
    "                                              \"solarVoltage_boum\" in col or \"radiation_category\" in col or \"direct_normal_irradiance\" in col])\n",
    "        value_vars = data_frame.columns.drop([\"month\", \"temperature_category\"])\n",
    "    return data_frame, value_vars\n",
    "\n",
    "\n",
    "# Melting and creating pivot tables for temperature and radiation data\n",
    "pivot_table_temperature, merged_df_temperature = melt_data(resulting_data, 'Temperature')\n",
    "pivot_table_radiation, merged_df_radiation = melt_data(resulting_data, 'Radiation')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c4246530daf1ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5: Visualization: Bar Plots for Temperature and Radiation Data\n",
    "*This section covers the creation of bar plots for temperature and radiation data.\n",
    "It details the setup of the plots, the use of seaborn for visualization,\n",
    "and the addition of annotations to enhance data interpretation.\n",
    "The process involves organizing data into subplots, coloring data by category,\n",
    "and applying layout adjustments before saving the plots as image files.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8cb0bb66db7eb84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_temperature_and_radiation_by_category(temperature_data, radiation_data):\n",
    "    \"\"\"\n",
    "    Plots temperature and radiation data by categories using bar plots.\n",
    "\n",
    "    Args:\n",
    "        temperature_data (pd.DataFrame):\n",
    "        DataFrame containing temperature data along with sensor and category information.\n",
    "        radiation_data (pd.DataFrame): DataFrame containing radiation data along with sensor and category information.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(24, 24))\n",
    "\n",
    "    # Plotting temperature data in the first subplot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    ax = sns.barplot(x=\"sensor\", y=\"temperature\", data=temperature_data, palette=\"icefire\",\n",
    "                     hue=\"temperature_category\", errorbar=None, errwidth=3, capsize=0.1, dodge=True, width=0.8)\n",
    "    plt.xlabel(\"Sensor\", fontsize=15)\n",
    "    ax.set_xticklabels([])  # Removing x-tick labels for clarity\n",
    "    plt.ylabel(\"Temperature [Â°C]\", fontsize=15)\n",
    "    plt.title(\"Temperature by Sensor and Category (Temperature Category)\", fontsize=20)\n",
    "    plt.legend(title=\"Temperature Category\", loc='upper right', fontsize=12)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()), ha=\"center\",\n",
    "            va=\"center\", fontsize=11, color=\"black\", xytext=(0, 5), textcoords=\"offset points\"\n",
    "        )\n",
    "\n",
    "    # Plotting radiation data in the second subplot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    ax = sns.barplot(x=\"sensor\", y=\"solar_voltage\", data=radiation_data, palette=\"icefire\",\n",
    "                     hue=\"radiation_category\", errorbar=None, errwidth=3, capsize=0.1, dodge=True, width=0.8)\n",
    "    plt.xlabel(\"Sensor\", fontsize=15)\n",
    "    plt.ylabel(\"Solar Voltage\", fontsize=15)\n",
    "    plt.title(\"Radiation by Sensor and Category (Radiation Category)\", fontsize=20)\n",
    "    plt.legend(title=\"Radiation Category\", loc='upper right', fontsize=12)\n",
    "    ax.set_xticklabels([])  # Removing x-tick labels for clarity\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.01f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()), ha=\"center\",\n",
    "            va=\"center\", fontsize=11, color=\"black\", xytext=(0, 5), textcoords=\"offset points\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/temperature_radiation_by_category.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_temperature_and_radiation_by_category(merged_df_temperature, merged_df_radiation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75b9a5b2f9422913"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 4: Data Clustering\n",
    "---\n",
    "*This section prepares data for clustering through cleaning, standardization,\n",
    "and dimensionality reduction via PCA, leading to clusters that reflect dataset patterns.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52734034cd0cf768"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Data Preprocessing and PCA Transformation for Clustering\n",
    "*This section of the notebook is dedicated to the preprocessing and PCA\n",
    "(Principal Component Analysis) transformation of data,\n",
    "setting the stage for effective clustering.\n",
    "The process involves several key steps:*\n",
    "\n",
    "1. **Melt Data Based on Mode:**\n",
    "Transforming the dataset from a wide to a long format,\n",
    "with each row representing a sensor measurement and categorized by the mode\n",
    "(such as temperature or radiation).\n",
    "\n",
    "2. **Data Cleaning:**\n",
    "Removing any non-numeric entries and handling missing values to ensure data integrity and consistency.\n",
    "\n",
    "3. **Standardizing Data:** Scaling the data so that each feature has a mean of zero and a standard deviation of one.\n",
    "This standardization is crucial for unbiased PCA and clustering analysis.\n",
    "\n",
    "4. **PCA and Scree Plot:** Performing Principal Component Analysis to reduce the dimensionality of the data.\n",
    "A scree plot is used to determine the optimal number of principal components to retain,\n",
    "striking a balance between simplifying the dataset and preserving its variance.\n",
    "\n",
    "5. **Outputting PCA-Transformed Data:** The final output of this step is the PCA-transformed data, ready for clustering."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c764b281cda4f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Cleans the data by selecting only numeric data and dropping missing values.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with only numeric data.\n",
    "    \"\"\"\n",
    "    return data.select_dtypes(include=[np.number]).dropna()\n",
    "\n",
    "\n",
    "def plot_scree_plot(data, mode):\n",
    "    \"\"\"\n",
    "    Plots the scree plot to determine the number of principal components to retain.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame whose PCA is to be performed.\n",
    "        mode (str): The mode of data ('Temperature' or 'Radiation').\n",
    "    \"\"\"\n",
    "    # Fit PCA and calculate variance ratios\n",
    "    pca = PCA(random_state=42).fit(data)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(1, len(explained_var) + 1), explained_var, alpha=0.5, align='center',\n",
    "            label='Individual explained variance')\n",
    "    plt.step(range(1, len(cumulative_var) + 1), cumulative_var, where='mid', label='Cumulative explained variance')\n",
    "    plt.xlabel('Principal Component', fontsize=15)\n",
    "    plt.ylabel('Explained Variance Ratio', fontsize=15)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"PCA Scree Plot for {mode} Data\", fontsize=20)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/pca_scree_plot_{mode}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def prepare_data_for_clustering(data_frame, n_components, mode):\n",
    "    \"\"\"\n",
    "    Performs data preprocessing and PCA transformation for clustering.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): The DataFrame to be processed.\n",
    "        n_components (float): The number of principal components to retain.\n",
    "        mode (str): The mode of data ('Temperature' or 'Radiation').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: PCA-transformed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_data = clean_data(data_frame)\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(cleaned_data)\n",
    "        plot_scree_plot(scaled_data, mode)\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        pca_transformed_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "        return pca_transformed_data, cleaned_data, scaler, pca\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data_for_clustering: {e}\")\n",
    "\n",
    "\n",
    "# Applying the function to temperature and radiation data\n",
    "pca_transformed_data_temperature, cleaned_data_temperature, scaler_temperature, pca_temperature = prepare_data_for_clustering(\n",
    "    pivot_table_temperature, n_components=0.95, mode=\"Temperature\")\n",
    "pca_transformed_data_radiation, cleaned_data_radiation, scaler_radiation, pca_radiation = prepare_data_for_clustering(\n",
    "    pivot_table_radiation, n_components=0.95, mode=\"Radiation\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6d03e906a8a7b94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comprehensive Data Clustering Pipeline for Sensor Data Analysis\n",
    "*This section outlines a detailed data clustering pipeline,\n",
    "which plays an important role in analyzing sensor data to uncover underlying patterns.\n",
    "The pipeline encompasses a range of critical functions,\n",
    "each contributing to different stages of the clustering process:*\n",
    "\n",
    "1. **Data Preprocessing:** Prepares the raw sensor data by cleaning and standardizing it,\n",
    "ensuring it is suitable for effective clustering.\n",
    "\n",
    "2. **PCA Transformation:** Implements Principal Component Analysis to reduce data dimensionality,\n",
    "making clustering more manageable and insightful.\n",
    "\n",
    "3. **Hierarchical Clustering and Dendrogram Plotting:**\n",
    "Applies hierarchical clustering to the data and visualizes the clustering hierarchy through dendrograms,\n",
    "providing a preliminary view of data groupings.\n",
    "\n",
    "4. **KMeans and DBSCAN Clustering:**\n",
    "Utilizes popular clustering algorithms like KMeans and DBSCAN to categorize the data into distinct clusters.\n",
    "\n",
    "5. **PCA Results Visualization:** Plots the results of PCA,\n",
    "offering a visual representation of data spread and cluster separations.\n",
    "\n",
    "6. **Model and Scaler Preservation:** Saves trained clustering models and scalers for future use,\n",
    "ensuring consistency in subsequent analyses.\n",
    "\n",
    "7. **Cluster Feature and Statistical Analysis:**\n",
    "Analyzes the features and computes statistical measures for each cluster,\n",
    "aiding in the understanding of each clusterâs characteristics.\n",
    "\n",
    "8. **Custom Functions for Modularity:** Incorporates custom functions for silhouette scoring, hierarchical clustering,\n",
    "dendrogram plotting, and cluster analysis, enhancing the codeâs modularity and readability.\n",
    "\n",
    "9. **Cluster Label Analysis and Visualization:** Involves reversing sensor-to-cluster mappings,\n",
    "updating cluster labels based on mean values,\n",
    "and plotting cluster label distributions alongside their statistical measures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca465f75284e6f90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_silhouette_scores(cluster_range, scores, mode):\n",
    "    \"\"\"\n",
    "    Plots the silhouette scores for a range of cluster counts.\n",
    "\n",
    "    Args:\n",
    "        cluster_range (range): The range of cluster counts to evaluate.\n",
    "        scores (list): Silhouette scores for each cluster count.\n",
    "        mode (str): 'Temperature' or 'Radiation', indicating the dataset type.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_range, scores, 'o-', color=\"black\")\n",
    "    mode_title = 'Temperature' if mode == 'Temperature' else 'Voltage'\n",
    "    plt.title(f\"Silhouette Method for Optimal Number of Clusters for {mode_title} Data\", fontsize=20)\n",
    "    plt.xlabel(\"Number of Clusters\", fontsize=15)\n",
    "    plt.ylabel(\"Silhouette Score\", fontsize=15)\n",
    "    plt.xticks(cluster_range, fontsize=15)\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/Silhouette_Method_Optimal_Number_of_Clusters_{mode}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_hierarchical_clustering(data, mode):\n",
    "    \"\"\"\n",
    "    Performs hierarchical clustering on the given dataset and plots a dendrogram.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Dataset for clustering.\n",
    "        mode (str): 'Temperature' or 'Radiation'.\n",
    "    \"\"\"\n",
    "    linkage_matrix = hierarchy.linkage(data, method=\"ward\")\n",
    "    plot_dendrogram(linkage_matrix, mode)\n",
    "\n",
    "\n",
    "def plot_dendrogram(linkage_matrix, mode):\n",
    "    \"\"\"\n",
    "    Plots a dendrogram for hierarchical clustering results.\n",
    "\n",
    "    Args:\n",
    "        linkage_matrix: Linkage matrix from hierarchical clustering.\n",
    "        mode (str): 'Temperature' or 'Radiation'.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hierarchy.dendrogram(linkage_matrix, orientation=\"top\", leaf_font_size=10)\n",
    "    mode_title = 'Temperature' if mode == 'Temperature' else 'Solar Radiation'\n",
    "    plt.xlabel(\"Sensor\", fontsize=15)\n",
    "    plt.ylabel(\"Distance\", fontsize=15)\n",
    "    plt.title(f\"Hierarchical Clustering Dendrogram for {mode_title} Data\", fontsize=20)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/hierarchical_clustering_{mode_title}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def apply_kmeans(data, mode, cluster_range=(2, 11)):\n",
    "    \"\"\"\n",
    "    Applies KMeans clustering to the dataset and plots silhouette scores.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Dataset for clustering.\n",
    "        mode (str): 'Temperature' or 'Radiation'.\n",
    "        cluster_range (tuple): Range of cluster numbers to try.\n",
    "\n",
    "    Returns:\n",
    "        KMeans object: Fitted KMeans model.\n",
    "    \"\"\"\n",
    "    silhouette_scores = []\n",
    "    for num_clusters in range(*cluster_range):\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=100).fit(data)\n",
    "        cluster_labels = kmeans.predict(data)\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    plot_silhouette_scores(range(*cluster_range), silhouette_scores, mode)\n",
    "    optimal_number_of_clusters = cluster_range[0] + silhouette_scores.index(max(silhouette_scores))\n",
    "    print(f\"Optimal number of clusters for {mode}: {optimal_number_of_clusters}\")\n",
    "    return KMeans(n_clusters=optimal_number_of_clusters, random_state=42, n_init=100).fit(data)\n",
    "\n",
    "\n",
    "def plot_pca_results(pca_transformed_data, cleaned_data, mode):\n",
    "    \"\"\"\n",
    "    Plots the results of PCA transformation with cluster labels.\n",
    "\n",
    "    Args:\n",
    "        pca_transformed_data (np.ndarray): PCA-transformed dataset.\n",
    "        cleaned_data (pd.DataFrame): DataFrame with cluster labels.\n",
    "        mode (str): 'Temperature' or 'Radiation'.\n",
    "    \"\"\"\n",
    "    pca_df = pd.DataFrame(data=pca_transformed_data[:, :2], columns=['PC1', 'PC2'])\n",
    "    pca_df['cluster'] = cleaned_data['cluster'].values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='cluster', data=pca_df, palette='colorblind')\n",
    "    plt.title(f'PCA of {mode} Data', fontsize=20)\n",
    "    plt.xlabel('Principal Component 1', fontsize=15)\n",
    "    plt.ylabel('Principal Component 2', fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.legend(title='Cluster', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/pca_results_{mode}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_clustering(data, cleaned_data, pca_transformed_data, mode):\n",
    "    \"\"\"\n",
    "    Performs clustering on the given dataset using KMeans.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Original dataset.\n",
    "        cleaned_data (pd.DataFrame): Cleaned dataset.\n",
    "        pca_transformed_data (np.ndarray): PCA-transformed data.\n",
    "        mode (str): 'Temperature' or 'Radiation'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Cluster model, cluster labels, and data.\n",
    "    \"\"\"\n",
    "    perform_hierarchical_clustering(pca_transformed_data, mode)\n",
    "\n",
    "    cluster_model = apply_kmeans(pca_transformed_data, mode)\n",
    "\n",
    "    cleaned_data['cluster'] = cluster_model.labels_\n",
    "    cluster_labels = cleaned_data[\"cluster\"].to_dict()\n",
    "\n",
    "    plot_pca_results(pca_transformed_data, cleaned_data, mode)\n",
    "\n",
    "    return cluster_model, cluster_labels, data\n",
    "\n",
    "\n",
    "# Apply the clustering function to temperature data\n",
    "temperature_clusters, temperature_cluster_labels, data_temperature = perform_clustering(\n",
    "    resulting_data, cleaned_data_temperature, pca_transformed_data_temperature,\n",
    "    mode=\"Temperature\")\n",
    "\n",
    "# Apply the clustering function to radiation data\n",
    "radiation_clusters, radiation_cluster_labels, data_radiation = perform_clustering(\n",
    "    resulting_data, cleaned_data_radiation, pca_transformed_data_radiation,\n",
    "    mode=\"Radiation\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0984e227b5459d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 5: Processing Clustered Data\n",
    "---\n",
    "*This section focuses on the post-clustering processing of data, including saving models for future inference,\n",
    "plotting cluster features, updating cluster labels, and analyzing cluster characteristics.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a84275ca3e5070df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1: Saving Trained Models and Scalers for Future Use\n",
    "*This code saves trained models and their scalers.\n",
    "This step is important for enabling these models to be later used for inference on new data,\n",
    "ensuring consistency in data scaling and model application.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d2d46b9174bf84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the trained models and scalers \n",
    "# This allows the scaler to be applied to new data that needs to be processed in the same way as the training data\n",
    "joblib.dump(scaler_temperature, \"../model/scaler_temperature.pkl\")\n",
    "joblib.dump(temperature_clusters, \"../model/temperature_clusters.pkl\")\n",
    "joblib.dump(pca_temperature, \"../model/pca_temperature.pkl\")\n",
    "\n",
    "joblib.dump(scaler_radiation, \"../model/scaler_radiation.pkl\")\n",
    "joblib.dump(radiation_clusters, \"../model/radiation_clusters.pkl\")\n",
    "joblib.dump(pca_radiation, \"../model/pca_radiation.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d958647e0e02993"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2: Analyzing Cluster Features and Statistical Measures\n",
    "*The code is dedicated to visualizing the characteristics of each cluster and computing key statistical measures.\n",
    "It includes plotting data for sensors in box plots and calculating metrics like mean, standard deviation, and range,\n",
    "providing a comprehensive statistical overview of each cluster.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60f290fe812c580b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "boum_data.set_index('timestamp', inplace=True)\n",
    "boum_data = boum_data[\n",
    "    (boum_data.index.month >= 4) & (boum_data.index.month <= 10)]  # Select the months April to October\n",
    "boum_data_org = boum_data.copy()\n",
    "boum_data = boum_data[\n",
    "    (boum_data.index.hour >= min_time) & (boum_data.index.hour <= max_time)]  # Select the maximum variability hours"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72bb349546a83588"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reverse_cluster_mapping(clustered_data):\n",
    "    \"\"\"\n",
    "    Reverses the mapping of sensors to clusters, creating a dictionary where each key is a cluster,\n",
    "    and the value is a list of sensors in that cluster.\n",
    "\n",
    "    Args:\n",
    "        clustered_data (dict): A dictionary mapping each sensor to its cluster.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with clusters as keys and lists of sensors as values.\n",
    "    \"\"\"\n",
    "    return {cluster: [sensor for sensor, cluster_id in clustered_data.items() if cluster_id == cluster] for cluster in\n",
    "            set(clustered_data.values())}\n",
    "\n",
    "\n",
    "def plot_cluster_features(sorted_clusters, result_data, mode=\"Temperature\"):\n",
    "    \"\"\"\n",
    "    Plots the features of each cluster.\n",
    "\n",
    "    Args:\n",
    "        sorted_clusters (dict): Dictionary with clusters as keys and lists of sensors as values.\n",
    "        result_data (pd.DataFrame): DataFrame containing the sensor data.\n",
    "        mode (str): Mode of data ('Temperature' for temperature or 'Radiation' for radiation).\n",
    "    \"\"\"\n",
    "    num_clusters = len(sorted_clusters)\n",
    "    grid_size = int(np.ceil(np.sqrt(num_clusters)))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 8), sharey=\"all\")\n",
    "    axes = axes.flatten()\n",
    "    result_data.index = pd.to_datetime(result_data.index)\n",
    "\n",
    "    for i, (cluster, sensors) in enumerate(sorted_clusters.items()):\n",
    "        if sensors:\n",
    "            cluster_data = result_data.loc[:, sensors]\n",
    "            sns.boxplot(data=cluster_data, ax=axes[i], palette=\"icefire\", showfliers=False)\n",
    "            axes[i].set_title(f\"Cluster {cluster}\")\n",
    "            axes[i].set_xticklabels([])\n",
    "            axes[i].set_xlabel(\"Sensor\")\n",
    "            axes[i].set_ylabel(\"Value\")\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, grid_size ** 2):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle(\"Cluster Features\" + (\" - Temperature\" if mode == \"Temperature\" else \" - Solar Radiation\"))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/cluster_features_{mode}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_cluster_statistics(sorted_clusters, result_data):\n",
    "    \"\"\"\n",
    "    Calculates statistical measures for each cluster.\n",
    "\n",
    "    Args:\n",
    "        sorted_clusters (dict): Dictionary with clusters as keys and lists of sensors as values.\n",
    "        result_data (pd.DataFrame): DataFrame containing the sensor data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the calculated statistics for each cluster.\n",
    "    \"\"\"\n",
    "    statistics = []\n",
    "    for cluster, sensors in sorted_clusters.items():\n",
    "        if sensors:\n",
    "            cluster_data = result_data.loc[:, sensors]\n",
    "            statistics.append({\n",
    "                \"Sensor\": sorted_clusters[cluster],\n",
    "                \"Cluster\": cluster,\n",
    "                \"Mean\": cluster_data.mean(axis=1).mean(),\n",
    "                \"Std\": cluster_data.std().mean(),\n",
    "                \"Max\": cluster_data.max().mean(),\n",
    "                \"Min\": cluster_data.min().mean(),\n",
    "                \"Range\": cluster_data.max().mean() - cluster_data.min().mean()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(statistics)\n",
    "\n",
    "\n",
    "def visualize_clusters(cluster_data, result_data, mode):\n",
    "    \"\"\"\n",
    "    Visualizes clusters and calculates their statistics.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (dict): A dictionary mapping each sensor to its cluster.\n",
    "        result_data (pd.DataFrame): DataFrame containing the sensor data.\n",
    "        mode (str): Mode of data ('Temperature' for temperature or 'Radiation' for radiation).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the calculated statistics for each cluster.\n",
    "    \"\"\"\n",
    "    if 'timestamp' in result_data.columns:\n",
    "        result_data.set_index('timestamp', inplace=True)\n",
    "\n",
    "    sorted_clusters = reverse_cluster_mapping(cluster_data)\n",
    "    if not sorted_clusters:\n",
    "        print(\"No clusters to visualize.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    plot_cluster_features(sorted_clusters, result_data, mode)\n",
    "    return calculate_cluster_statistics(sorted_clusters, result_data)\n",
    "\n",
    "\n",
    "df_temperature_all = visualize_clusters(temperature_cluster_labels, boum_data, \"Temperature\")\n",
    "df_radiation_all = visualize_clusters(radiation_cluster_labels, boum_data, \"Radiation\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fafa08a1f4d05a9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*set labels*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7207da7f70ef1348"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temperature_labels = [\"cool\", \"warm\", \"hot\"]\n",
    "radiation_labels = [\"dark\", \"medium dark\", \"medium bright\", \"bright\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f29eb5e0c84f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4: Updating Cluster Labels Based on Mean Values\n",
    "*This segment updates cluster labels in a DataFrame based on their mean values.\n",
    "By sorting the DataFrame, mapping means to the labels, and updating labels, it adds interpretive value to the clusters,\n",
    "helping in the identification of patterns and anomalies.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "229fc62ad79c43cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_cluster_labels(data_frame, labels):\n",
    "    \"\"\"\n",
    "    Updates the cluster labels in the DataFrame based on their mean values.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing cluster statistics.\n",
    "        labels (list): List of labels to assign to each cluster.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Original DataFrame and the updated DataFrame with new cluster labels.\n",
    "    \"\"\"\n",
    "    if len(data_frame[\"Cluster\"].unique()) != len(labels):\n",
    "        raise ValueError(\"Number of labels does not match the number of clusters.\")\n",
    "\n",
    "    # Sorting the DataFrame by the mean values of each cluster\n",
    "    sorted_clusters = data_frame.sort_values(by=\"Mean\", ascending=True)[\"Cluster\"].unique()\n",
    "    # Creating a mapping from original cluster labels to new labels\n",
    "    label_mapping = {cluster: label for cluster, label in zip(sorted_clusters, labels)}\n",
    "    print(\"Cluster to Label Mapping:\", label_mapping)\n",
    "\n",
    "    # Applying the new labels to the DataFrame\n",
    "    data_frame['new_label'] = data_frame['Cluster'].map(label_mapping)\n",
    "    original_data_frame = data_frame.copy()\n",
    "    data_frame = data_frame.explode(\"Sensor\")\n",
    "\n",
    "    return original_data_frame, data_frame, label_mapping\n",
    "\n",
    "\n",
    "# Updating cluster labels for temperature and radiation data\n",
    "df_temperature_all_org, df_temperature_all_updated, label_mapping_temperature = update_cluster_labels(\n",
    "    df_temperature_all, temperature_labels)\n",
    "df_radiation_all_org, df_radiation_all_updated, label_mapping_radiation = update_cluster_labels(df_radiation_all,\n",
    "                                                                                                radiation_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0293f94343bbc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6: Plotting Cluster Label Distribution and Statistical Analysis\n",
    "*This section plots the distribution of cluster labels and calculates statistical measures for each cluster.\n",
    "It employs functions like `plot_label_distribution` for visualization\n",
    "and `calculate_cluster_statistics` for measuring key statistical parameters,\n",
    "thereby offering a detailed insight into the cluster distribution and characteristics.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47a98fc52c6e18b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_label_distribution(df, column, labels, title, ax):\n",
    "    \"\"\"\n",
    "    Plots the distribution of labels in a given DataFrame column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        column (str): The column in the DataFrame to plot.\n",
    "        labels (list): List of labels to include in the plot.\n",
    "        title (str): Title of the plot.\n",
    "        ax (matplotlib.axes._subplots.AxesSubplot): The axes on which to plot.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    sns.countplot(x=column, data=df, order=labels, ax=ax, alpha=1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Cluster Labels')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "\n",
    "def plot_distribution():\n",
    "    \"\"\"\n",
    "    Plots the distribution of temperature and radiation cluster labels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    plot_label_distribution(df_temperature_all_updated, 'new_label', temperature_labels,\n",
    "                            'Temperature Cluster Distribution', axes[0])\n",
    "    plot_label_distribution(df_radiation_all_updated, 'new_label', radiation_labels, 'Radiation Cluster Distribution',\n",
    "                            axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/cluster_labels.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4511998c2ee7b3b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the number of locations in each cluster.\n",
    "print(df_temperature_all_updated.value_counts('new_label'))\n",
    "print(df_radiation_all_updated.value_counts('new_label'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58c1596bd95a132b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Define temperature_columns and radiation_columns*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376738392238a8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if temperature_columns != [col for col in df_temperature_all_updated[\"Sensor\"] if col.startswith(\"temperature_boum\")]:\n",
    "    temperature_columns = [col for col in df_temperature_all_updated[\"Sensor\"] if col.startswith]\n",
    "    voltage_columns = [col.replace(\"temperature\", \"solarVoltage\") for col in temperature_columns]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3e37341d1f9558"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 6: Cluster Features and Statistics Analysis\n",
    "---\n",
    "*This section calculates statistical measures for each cluster,\n",
    "visualizes label distribution, and examines cluster features in depth.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349f693b58d4294e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1: Calculating Statistical Measures for Each Cluster\n",
    "*Focuses on the computation of statistical measures for each cluster.\n",
    "This step is pivotal in understanding the underlying data distribution and cluster characteristics.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1736848d1dee4d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_new_cluster_statistics(df, data_all, data_columns, result_prefix):\n",
    "    \"\"\"\n",
    "    Calculate cluster statistics for given data columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing data columns.\n",
    "        data_all (pd.DataFrame): DataFrame containing cluster information.\n",
    "        data_columns (list): List of data columns to calculate statistics.\n",
    "        result_prefix (str): Prefix to add to result column names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with calculated statistics.\n",
    "    \"\"\"\n",
    "    # Calculate statistics for data columns\n",
    "    statistics = df[data_columns].agg(\n",
    "        [\"min\", \"max\", \"mean\", \"std\"]).transpose()\n",
    "    statistics.columns = [f\"{result_prefix}_{col}\" for col in statistics.columns]\n",
    "    statistics = statistics.reset_index().rename(columns={\"index\": \"Sensor\"})\n",
    "\n",
    "    # Merge with cluster information\n",
    "    statistics = pd.merge_ordered(statistics, data_all[[\"Sensor\", \"Cluster\", \"new_label\"]], on=\"Sensor\").rename(\n",
    "        columns={\"Cluster\": f\"Cluster_{result_prefix}\", \"new_label\": f\"Cluster_{result_prefix}_label\"})\n",
    "    statistics[\"Sensor\"] = statistics[\"Sensor\"].str[-8:]\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def calculate_monthly_cluster_statistics(data_frame, month_column, months):\n",
    "    \"\"\"\n",
    "    Calculate monthly cluster statistics for given data columns.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing data columns.\n",
    "        month_column (str): Column containing month information.\n",
    "        months (list): List of months to calculate statistics for.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DataFrames containing monthly statistics.\n",
    "    \"\"\"\n",
    "    monthly_data_frames = []\n",
    "    for month in months:\n",
    "        month_data = data_frame[data_frame[month_column] == month]\n",
    "        if not month_data.empty:\n",
    "            df_temp_month = calculate_new_cluster_statistics(\n",
    "                month_data,\n",
    "                df_temperature_all_updated,\n",
    "                temperature_columns,\n",
    "                f\"temperature_{month}\",\n",
    "            )\n",
    "            df_dni_month = calculate_new_cluster_statistics(\n",
    "                month_data,\n",
    "                df_radiation_all_updated,\n",
    "                voltage_columns,\n",
    "                f\"voltage_{month}\",\n",
    "            )\n",
    "            df_combined_month = pd.merge_ordered(df_temp_month, df_dni_month, on=\"Sensor\")\n",
    "            monthly_data_frames.append(df_combined_month)\n",
    "        else:\n",
    "            print(f\"No data for month {month}\")\n",
    "    return monthly_data_frames\n",
    "\n",
    "\n",
    "# Calculate cluster statistics for temperature and solar data\n",
    "df_combined_temperature = calculate_new_cluster_statistics(\n",
    "    boum_data, df_temperature_all_updated, temperature_columns, \"temperature\"\n",
    ")\n",
    "df_combined_solar = calculate_new_cluster_statistics(\n",
    "    boum_data, df_radiation_all_updated, voltage_columns, \"voltage\"\n",
    ")\n",
    "\n",
    "# Calculate monthly statistics\n",
    "months_to_calculate = range(4, 11)\n",
    "df_combined_all = calculate_monthly_cluster_statistics(boum_data, month_column=\"m\", months=months_to_calculate)\n",
    "\n",
    "# Merge temperature and solar statistics\n",
    "df_combined = pd.merge_ordered(df_combined_temperature, df_combined_solar, on=\"Sensor\").dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14542c8b4510802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 6.1: Visualizing Cluster Features\n",
    "*This part of the analysis involves the visualization of cluster features,\n",
    "including the creation of regression plots with scatter points and regression lines for each cluster.\n",
    "The process includes plotting and fitting linear regression models to understand the relationships within the data.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffa486a629d04f57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1.1: Regression Analysis for Each Cluster\n",
    "*This section details creating regression plots with scatter points and regression lines for each cluster.\n",
    "The process includes plotting scatter points, fitting a linear regression model with statsmodels,\n",
    "and then annotating the plot with the line equation, slope, and intercept.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f1ed53950e9ae23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_regression(data_frame, x_column, y_column, hue_column, style_column, title=\"Regression Plot\"):\n",
    "    \"\"\"\n",
    "    Plot a regression plot with scatter points and a regression line.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing the data.\n",
    "        x_column (str): Column for the x-axis.\n",
    "        y_column (str): Column for the y-axis.\n",
    "        hue_column (str): Column for hue (color).\n",
    "        style_column (str): Column for style (marker style).\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(\n",
    "        data=data_frame, x=x_column, y=y_column, hue=hue_column, style=style_column, legend=\"brief\",\n",
    "        palette=\"colorblind\"\n",
    "    )\n",
    "    model = LinearRegression()\n",
    "    X = data_frame[[x_column]]\n",
    "    Y = data_frame[y_column]\n",
    "    model.fit(X, Y)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    sns.regplot(data=data_frame, x=x_column, y=y_column, scatter=False, color=\"black\", ci=95, ax=plt.gca())\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Mean Voltage [V]')\n",
    "    plt.xlabel('Mean Temperature [Â°C]')\n",
    "    equation_text = f\"Slope: {slope:.2f}\\nIntercept: {intercept:.2f}\"\n",
    "    plt.annotate(equation_text, xy=(0.88, 0.04), xycoords=\"axes fraction\", size=12,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/{title}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_regression(df_combined, \"temperature_mean\", \"voltage_mean\", \"Cluster_voltage_label\", \"Cluster_temperature_label\",\n",
    "                title=\"Regression Plot\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8242fd7465bff6b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1.2: Monthly Regression Analysis and Cluster Statistics\n",
    "*Focusing on monthly data, this code plots scatter plots with regression lines for each month,\n",
    "fitting linear regression models to discern patterns.\n",
    "It includes error handling for unsuccessful model fits, and annotation of plots with slope and R-squared values,\n",
    "offering an examination of monthly trends.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9f138a5d7b4bdc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MONTH_NAMES = [\"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\"]\n",
    "MONTH_RANGE = range(4, 11)\n",
    "\n",
    "\n",
    "def plot_cluster_scatter(data_frame):\n",
    "    \"\"\"\n",
    "    Plot cluster scatter plots by month.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(12, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, month in enumerate(MONTH_RANGE):\n",
    "        data_frame_month = data_frame[i]\n",
    "\n",
    "        temp_col = f\"temperature_{month}_mean\"\n",
    "        solar_col = f\"voltage_{month}_mean\"\n",
    "        temp_cluster_col = f\"Cluster_temperature_{month}_label\"\n",
    "        solar_cluster_col = f\"Cluster_voltage_{month}_label\"\n",
    "        data_frame_month[temp_col] = pd.to_numeric(data_frame_month[temp_col], errors=\"coerce\")\n",
    "        data_frame_month[solar_col] = pd.to_numeric(data_frame_month[solar_col], errors=\"coerce\")\n",
    "\n",
    "        month_data = data_frame_month.dropna(subset=[temp_col, solar_col])\n",
    "        fig.suptitle(\"Cluster Scatter Plot by Month\\n\\n\\n\", fontsize=16, y=0.93)\n",
    "\n",
    "        if not month_data.empty:\n",
    "            X = sm.add_constant(month_data[temp_col])\n",
    "            y = month_data[solar_col]\n",
    "\n",
    "            try:\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                slope = model.params[1]\n",
    "                r_squared = model.rsquared\n",
    "\n",
    "                sns.scatterplot(data=month_data, x=temp_col, y=solar_col, hue=temp_cluster_col, style=solar_cluster_col,\n",
    "                                ax=axs[i], legend=False)\n",
    "\n",
    "                sns.regplot(data=month_data, x=temp_col, y=solar_col, ax=axs[i], scatter=False, color=\"black\")\n",
    "\n",
    "                axs[i].annotate(f\"Slope: {slope:.2f}\\nRÂ²: {r_squared:.2f}\", xy=(0.95, 0.05), xycoords=\"axes fraction\",\n",
    "                                ha=\"right\", va=\"bottom\", size=10,\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "\n",
    "            except Exception as excep:\n",
    "                print(f\"Error in regression model for month {month}: {excep}\")\n",
    "                axs[i].set_title(f\"{MONTH_NAMES[i - 1]} (model not fit)\")\n",
    "            axs[i].set_title(f\"{MONTH_NAMES[i - 1]}\")\n",
    "            axs[i].grid(True)\n",
    "\n",
    "        else:\n",
    "            axs[i].set_title(f\"{MONTH_NAMES[i - 1]} (no data)\")\n",
    "            fig.delaxes(axs[i])  # Remove axis if no data\n",
    "\n",
    "    last_ax = axs[-1]\n",
    "\n",
    "    sns.scatterplot(data=month_data, x=temp_col, y=solar_col, hue=temp_cluster_col, ax=last_ax,\n",
    "                    alpha=0)\n",
    "\n",
    "    sns.scatterplot(data=month_data, x=temp_col, y=solar_col, style=solar_cluster_col, ax=last_ax,\n",
    "                    alpha=0)\n",
    "\n",
    "    handles, labels = last_ax.get_legend_handles_labels()\n",
    "    temp_handles, temp_labels = handles[:len(temperature_labels)], labels[:len(temperature_labels)]\n",
    "    radiation_handles, rad_labels = handles[len(temperature_labels):], labels[len(temperature_labels):]\n",
    "\n",
    "    last_ax.legend(temp_handles + radiation_handles,\n",
    "                   [\"Temperature: \" + label for label in temp_labels] + [\"Solar Voltage: \" + label for label in\n",
    "                                                                         rad_labels], loc='center',\n",
    "                   title=\"Legend\", fontsize=10.5, ncol=2)\n",
    "\n",
    "    last_ax.axis('off')\n",
    "\n",
    "    for i, ax in enumerate(axs[:-1]):\n",
    "        ax.set_ylabel(\"Mean Solar Voltage [V]\")\n",
    "        ax.set_xlabel(\"Mean Temperature [Â°C]\")\n",
    "        ax.set_title(MONTH_NAMES[i])\n",
    "\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/cluster_scatter.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cluster_scatter(df_combined_all)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7110f38d1380ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1.3: Correlation Analysis Between Temperature and Solar Data\n",
    "*This part of the analysis calculates and visualizes the correlation between temperature and solar data.\n",
    "The code uses pandas to compute the correlation matrix and seaborn to create a heatmap,\n",
    "illustrating the relationship between these two key data points.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d26b1f42b80fb25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(data_frame, columns, title=\"Correlation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Plot a correlation heatmap for selected columns.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): DataFrame containing the data.\n",
    "        columns (list): List of column names to include in the heatmap.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    correlation_matrix = data_frame[columns].corr().round(2)\n",
    "    correlation, p_value = pearsonr(data_frame[columns[0]], data_frame[columns[1]])\n",
    "    print('Correlation:', correlation, 'P-value:', p_value)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"icefire\", vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/correlation_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(df_combined, [\"temperature_mean\", \"voltage_mean\"], title=\"Correlation Heatmap\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90283a8dafb9a705"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2: Mapping Cluster Labels to Numerical Values\n",
    "*This section describes the process of converting cluster labels into numerical values for easier analysis.\n",
    "It involves mapping labels to integers and adjusting DataFrame indices,\n",
    "thus preparing the data for further numerical analysis and visualization.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f6c79332fb1c4ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def map_clusters(df_temp, df_dni):\n",
    "    \"\"\"\n",
    "    Map cluster labels to numerical values for temperature and DNI DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df_temp (DataFrame): DataFrame containing temperature data with 'Cluster' column.\n",
    "        df_dni (DataFrame): DataFrame containing DNI data with 'Cluster' column.\n",
    "\n",
    "    Returns:\n",
    "        df_temp (DataFrame): Updated temperature DataFrame with numerical cluster values.\n",
    "        df_dni (DataFrame): Updated DNI DataFrame with numerical cluster values.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_mapping_temp = {\"cool\": 0, \"warm\": 1, \"hot\": 2}\n",
    "    df_temp[\"Cluster\"] = df_temp[\"new_label\"].map(cluster_mapping_temp)\n",
    "\n",
    "    cluster_mapping_dni = {\"dark\": 0, \"medium dark\": 1, \"medium bright\": 2, \"bright\": 3}\n",
    "    df_dni[\"Cluster\"] = df_dni[\"new_label\"].map(cluster_mapping_dni)\n",
    "\n",
    "    df_dni.set_index(\"Sensor\", inplace=True)\n",
    "    df_temp.set_index(\"Sensor\", inplace=True)\n",
    "\n",
    "    return df_temp, df_dni\n",
    "\n",
    "\n",
    "df_temperature_all_updated, df_radiation_all_updated = map_clusters(df_temperature_all_updated,\n",
    "                                                                    df_radiation_all_updated)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4dcc22a116ffd8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3: Heatmap Visualization of Cluster Statistics\n",
    "*This code creates heatmaps to visualize cluster statistics for temperature and DNI data.\n",
    "It involves grouping data by cluster value, calculating mean group values,\n",
    "and uses seaborn to plot the resultant heatmaps, offering a clear and concise visualization of cluster distributions.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a27fe7e154af555"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_cluster_heatmap(data_frame, plot_title, axis, decimal_places=1):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of cluster values for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_frame (DataFrame): The DataFrame containing cluster values.\n",
    "        plot_title (str): The title for the heatmap.\n",
    "        axis (matplotlib.axes._subplots.AxesSubplot): The axis to plot the heatmap.\n",
    "        decimal_places (int, optional): The number of decimal places for annotations (default is 1).\n",
    "    \"\"\"\n",
    "    sns.heatmap(data_frame.groupby(\"new_label\").mean(), annot=True, cmap=\"icefire\", fmt=f\".{decimal_places}f\",\n",
    "                cbar=False,\n",
    "                ax=axis, annot_kws={\"size\": 20})\n",
    "    axis.set_title(plot_title, fontsize=20)\n",
    "    axis.set_xticklabels(axis.get_xticklabels(), fontsize=20)\n",
    "    axis.set_yticklabels(axis.get_yticklabels(), fontsize=20)\n",
    "\n",
    "\n",
    "def plot_cluster_heatmaps():\n",
    "    \"\"\"\n",
    "    Plot heatmaps for temperature and DNI cluster values.\n",
    "    \"\"\"\n",
    "    fig, (axis0, axis1) = plt.subplots(1, 2, figsize=(24, 12))\n",
    "    plot_cluster_heatmap(df_temperature_all_updated, \"Temperature Cluster Values Heatmap\", decimal_places=1, axis=axis0)\n",
    "    plot_cluster_heatmap(df_radiation_all_updated, \"DNI Cluster Values Heatmap\", decimal_places=1, axis=axis1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cluster_heatmaps()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8325bd76590fb1eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 7: Analyzing Survey Data\n",
    "---\n",
    "*This segment focuses on merging cluster data with survey data,\n",
    "conducting correlation analysis, and exploring cluster-related correlations in both datasets.\n",
    "It encompasses preprocessing, correlation plotting,\n",
    "and data merging to facilitate a comprehensive analysis of survey responses.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe136e6c88f1b083"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Define columns of interest in the survey data.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37ca9ab5bd061ff6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic Information\n",
    "basic_info = [\n",
    "    \"orientation_balcony_short\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"floor\",\n",
    "]\n",
    "\n",
    "# Tank and Sunlight\n",
    "tank_sunlight = [\n",
    "    \"tank_position\",\n",
    "    \"tank_location\",\n",
    "    \"sunshine_window_user\",\n",
    "]\n",
    "\n",
    "# Orientation and Climate\n",
    "orientation_climate = [\n",
    "    \"compass_degree\",\n",
    "    \"Cluster\",\n",
    "    \"rating_climate_user\",\n",
    "]\n",
    "\n",
    "# Building Features\n",
    "building_features = [\n",
    "    \"blinds_frequency\",\n",
    "    \"railing_transparancy\",\n",
    "]\n",
    "\n",
    "# Location and Population\n",
    "location_population = [\n",
    "    \"population\",\n",
    "    \"type\",\n",
    "]\n",
    "\n",
    "# Combine all categories into one list\n",
    "columns_of_interest = (\n",
    "        basic_info\n",
    "        + tank_sunlight\n",
    "        + orientation_climate\n",
    "        + building_features\n",
    "        + location_population\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f3507f9af643a13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1: Merging Cluster and Survey Data\n",
    "*This procedure merges cluster data with survey responses.\n",
    "It is designed to preprocess data for temperature and solar voltage,\n",
    "align survey responses with cluster data, and rename columns for clarity,\n",
    "ultimately enabling a thorough analysis of merged data.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c7eb3aa7e5652ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_cluster_data(cluster_data, mode):\n",
    "    \"\"\"\n",
    "    Preprocesses cluster data by merging it with survey data and filtering based on survey responses.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (pd.DataFrame): Cluster data for temperature or solar voltage.\n",
    "        mode (str): Either \"Temperature\" for temperature or \"Radiation\" for solar voltage.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed and filtered data.\n",
    "    \"\"\"\n",
    "    # Get survey data and define a residency type\n",
    "    survey_df = survey_data_instance.get_survey_data()\n",
    "    survey_df = define_residency_type(survey_df)\n",
    "    survey_df.set_index('deviceId_boum', inplace=True)\n",
    "    survey_df = survey_df.T\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    names = [\n",
    "        f\"temperature_boum_{col[:8]}_boum_{col[:8]}\" if mode == \"Temperature\" else f\"solarVoltage_boum_{col[:8]}_boum_{col[:8]}\"\n",
    "        for col in survey_df.columns]\n",
    "\n",
    "    survey_df.columns = names\n",
    "    survey_df = survey_df.T.reset_index(names='Sensor')\n",
    "    merged_df = pd.merge_ordered(survey_df, cluster_data, on='Sensor')\n",
    "    merged_df = merged_df[(merged_df['answered_message'] == True) | (merged_df['did_survey'] == True)]\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def define_residency_type(survey_df):\n",
    "    \"\"\"\n",
    "    Defines the residency type based on population.\n",
    "\n",
    "    Args:\n",
    "        survey_df (pd.DataFrame): Survey data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Survey data with \"type\" column indicating a residency type.\n",
    "    \"\"\"\n",
    "    survey_df[\"type\"] = \"urban\"\n",
    "    survey_df.loc[survey_df[\"population\"] < 15000, \"type\"] = \"rural\"\n",
    "    return survey_df\n",
    "\n",
    "\n",
    "# Preprocess cluster data for temperature and solar voltage\n",
    "merged_df_temp = preprocess_cluster_data(df_temperature_all_updated[\"Cluster\"], mode=\"Temperature\")\n",
    "merged_df_dni = preprocess_cluster_data(df_radiation_all_updated[\"Cluster\"], mode=\"Radiation\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "590e5e606578aff7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2: Correlation Analysis in Cluster and Survey Data\n",
    "*Here, the focus is on statistically relevant correlations within the data.\n",
    "The code performs correlation analysis using pandas and seaborn,\n",
    "filtering for significant correlations and visualizing these relationships in a heatmap,\n",
    "thereby highlighting key insights and patterns in the data.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfe83f785de72cd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_statistically_relevant_correlations(dataframe, columns, p_value_threshold, correlation_threshold, mode):\n",
    "    \"\"\"\n",
    "    Plot statistically relevant correlations between features in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe.\n",
    "        columns (list): List of columns to consider for correlation analysis.\n",
    "        p_value_threshold (float): Threshold for p-value to determine statistical significance.\n",
    "        correlation_threshold (float): Threshold for correlation coefficient to consider relevant.\n",
    "        mode (str): Either 'Temperature' or 'Radiation' indicating the mode.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered correlation matrix.\n",
    "    \"\"\"\n",
    "    data = dataframe[columns]\n",
    "    data = pd.get_dummies(data, columns=[\"orientation_balcony_short\", \"tank_position\", \"tank_location\",\n",
    "                                         \"sunshine_window_user\", \"blinds_frequency\", \"Cluster\",\n",
    "                                         \"railing_transparancy\", \"type\"])\n",
    "\n",
    "    correlation_matrix = data.corr(method='pearson')\n",
    "    p_value_matrix = data.corr(method=lambda x, y: pearsonr(x, y)[1])\n",
    "    filtered_correlation_matrix = correlation_matrix[(abs(p_value_matrix) < p_value_threshold) & (\n",
    "            abs(correlation_matrix) > correlation_threshold\n",
    "    )].dropna(thresh=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(filtered_correlation_matrix, annot=True, cmap=\"icefire\", fmt=\".1f\", annot_kws={\"size\": 12})\n",
    "    plt.title(\"Statistically Relevant Correlations\")\n",
    "    plt.xlabel(\"Correlation Coefficient\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/statistically_relevant_correlations_{mode}.png\")\n",
    "    plt.show()\n",
    "    return filtered_correlation_matrix, p_value_matrix\n",
    "\n",
    "\n",
    "filtered_correlation_matrix_temp, p_value_matrix_temp = plot_statistically_relevant_correlations(\n",
    "    merged_df_temp, columns_of_interest, p_value_threshold=0.05, correlation_threshold=0, mode='Temperature'\n",
    ")\n",
    "filtered_correlation_matrix_dni, p_value_matrix_dni = plot_statistically_relevant_correlations(\n",
    "    merged_df_dni, columns_of_interest, p_value_threshold=0.05, correlation_threshold=0, mode='Radiation'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d4e954c343cdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.3: Cluster-Related Correlation Analysis\n",
    "*This code isolates cluster-related correlations for a detailed examination.\n",
    "It processes the correlation matrix to separate cluster-related columns\n",
    "and then concatenates these columns into a new DataFrame,\n",
    "providing a focused view of cluster-specific relationships within the data.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c00bb925e7b9e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_correlation_results(correlation_matrix, cluster_prefix='Cluster'):\n",
    "    \"\"\"\n",
    "    Process the correlation matrix by filtering out cluster-related columns.\n",
    "    \n",
    "    Args:\n",
    "        correlation_matrix (pd.DataFrame): The correlation matrix to process.\n",
    "        cluster_prefix (str, optional): The prefix used for cluster-related columns.\n",
    "        Default is 'Cluster'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed correlation matrix with cluster-related columns separated.\n",
    "    \"\"\"\n",
    "    cluster_index = [i for i in correlation_matrix.index if cluster_prefix in i]\n",
    "    correlation_matrix = correlation_matrix.drop(index=cluster_index)\n",
    "    cluster_columns = [col for col in correlation_matrix.columns if col.startswith(cluster_prefix)]\n",
    "    correlation_result = pd.DataFrame()\n",
    "    for col in cluster_columns:\n",
    "        correlation_result = pd.concat([correlation_result, correlation_matrix[col].dropna()], axis=1)\n",
    "    return correlation_result\n",
    "\n",
    "\n",
    "corr_results_temperature = process_correlation_results(filtered_correlation_matrix_temp)\n",
    "corr_results_radiation = process_correlation_results(filtered_correlation_matrix_dni)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b54cfa1750d069c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.4: Visualizing Correlations in Cluster and Survey Data\n",
    "*This section plots statistically significant correlations between various features in the dataset.\n",
    "The approach involves filtering correlations based on p-values and correlation coefficients,\n",
    "creating dummy variables for categorical data,\n",
    "and visualizing the results in a heatmap, providing a clear understanding of feature relationships.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb7dc5f3c9707de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_cluster_correlations(corr_results, p_value_matrix, title, cluster_labels, colors, alpha=0.7,\n",
    "                              mode='Temperature'):\n",
    "    \"\"\"\n",
    "    Plot cluster correlations for different clusters.\n",
    "\n",
    "    Args:\n",
    "        corr_results (pd.DataFrame): The DataFrame containing cluster correlations.\n",
    "        p_value_matrix (pd.DataFrame): The DataFrame containing cluster p-values.\n",
    "        title (str): The title for the plot.\n",
    "        cluster_labels (list): A list of cluster labels.\n",
    "        colors (list): A list of colors for the clusters.\n",
    "        alpha (float, optional): The transparency of the bars.\n",
    "        Default is 0.7.\n",
    "        mode (str, optional): The mode of the correlations (e.g., 'Temperature' or 'Radiation').\n",
    "        Default is 'Temperature'.\n",
    "    \"\"\"\n",
    "    # Edit p-values\n",
    "    for col in p_value_matrix.columns:\n",
    "        for ro in p_value_matrix.index:\n",
    "            p_value = p_value_matrix.at[ro, col]\n",
    "            if pd.notnull(p_value) and np.isfinite(p_value):\n",
    "                if p_value <= 0.01:\n",
    "                    p_value_matrix.at[ro, col] = 0.01\n",
    "                elif 0.01 < p_value <= 0.05:\n",
    "                    p_value_matrix.at[ro, col] = 0.05\n",
    "                elif 0.05 < p_value <= 0.1:\n",
    "                    p_value_matrix.at[ro, col] = 0.1\n",
    "                else:\n",
    "                    p_value_matrix.at[ro, col] = None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    for i, (label, color) in enumerate(zip(cluster_labels, colors), start=1):\n",
    "        cluster_col = f\"Cluster_{i - 1}.0\"\n",
    "\n",
    "        if cluster_col in corr_results:\n",
    "            bar_plot = sns.barplot(\n",
    "                data=corr_results,\n",
    "                y=corr_results.index,\n",
    "                x=cluster_col,\n",
    "                color=color,\n",
    "                label=label,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        for bar, feature in zip(bar_plot.patches, corr_results.index):\n",
    "            p_value = p_value_matrix.at[feature, cluster_col]\n",
    "            value = corr_results.at[feature, cluster_col]\n",
    "            if pd.notnull(p_value) and np.isfinite(p_value):\n",
    "                text_x = -0.2 if value < 0 else 0.2\n",
    "                ax.text(text_x, bar.get_y() + bar.get_height() / 2, f\"p = {p_value:.2f}\",\n",
    "                        va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Correlation Coefficient\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/cluster_correlations_{mode}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the color palettes\n",
    "radiation_colors = sns.color_palette('colorblind')\n",
    "temperature_colors = sns.color_palette('colorblind')\n",
    "\n",
    "# Call the function to plot temperature and radiation cluster correlations\n",
    "plot_cluster_correlations(\n",
    "    corr_results_temperature,\n",
    "    p_value_matrix_temp,\n",
    "    \"Temperature Clusters\",\n",
    "    temperature_labels,\n",
    "    temperature_colors,\n",
    "    0.7,\n",
    "    'Temperature'\n",
    ")\n",
    "plot_cluster_correlations(\n",
    "    corr_results_radiation,\n",
    "    p_value_matrix_dni,\n",
    "    \"Radiation Clusters\",\n",
    "    radiation_labels,\n",
    "    radiation_colors,\n",
    "    0.7,\n",
    "    'Radiation'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365b22e07491262f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 8: Analyzing Cluster Data\n",
    "*Here,\n",
    "the code analyzes the cluster-related correlations\n",
    "and visualizes the results in a heatmap to aid in understanding of feature relationships within the data.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ba232213d74f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.1: Calculating Mean Cluster Values\n",
    "*Here, the mean values for each cluster are analyzed, combining the cluster data with the original sensor data.\n",
    "The process involves selecting relevant sensor data for each cluster, computing mean values,\n",
    "and creating a new DataFrame to house these calculations,\n",
    "offering insights into the average characteristics of each cluster.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f66f98929b62a60b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_cluster_mean(data, org_data):\n",
    "    \"\"\"\n",
    "    Calculate the mean of each cluster for the given data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The data containing cluster information.\n",
    "        org_data (pd.DataFrame): The original data with sensor information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the mean values for each cluster.\n",
    "    \"\"\"\n",
    "    cluster_means = {}\n",
    "    unique_clusters = org_data['new_label'].unique()\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_sensors = org_data[org_data['new_label'] == cluster]['Sensor'].explode()\n",
    "        cluster_means[f\"Cluster {cluster}\"] = data[cluster_sensors].mean(axis=1)\n",
    "    return pd.DataFrame(cluster_means)\n",
    "\n",
    "\n",
    "mean_all_cluster_temperature = calculate_cluster_mean(boum_data, df_temperature_all_org)\n",
    "mean_all_cluster_radiation = calculate_cluster_mean(boum_data, df_radiation_all_org)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "342e10b90bc71d57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.2: Merging Cluster and Original Data\n",
    "*This section merges cluster data with original sensor data.\n",
    "The process includes selecting sensor data corresponding to each cluster,\n",
    "calculating mean values, and compiling these calculations in a new DataFrame.\n",
    "This step is important\n",
    "for understanding the average characteristics of each cluster in the context of the original dataset.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc4c7434063ebc98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_all_cluster_data(df_dni_all_updated, df_temp_all_updated):\n",
    "    \"\"\"\n",
    "    Merge and organize cluster data from temperature and DNI datasets.\n",
    "\n",
    "    Args:\n",
    "        df_dni_all_updated (pd.DataFrame): The DNI dataset with cluster information.\n",
    "        df_temp_all_updated (pd.DataFrame): The temperature dataset with cluster information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame containing cluster data.\n",
    "    \"\"\"\n",
    "    df_dni_all_updated.reset_index(inplace=True)\n",
    "    df_temp_all_updated.reset_index(inplace=True)\n",
    "    df_temp_all_updated[\"Sensor\"] = df_temp_all_updated[\"Sensor\"].str[-8:]\n",
    "    df_dni_all_updated[\"Sensor\"] = df_dni_all_updated[\"Sensor\"].str[-8:]\n",
    "    data_all = pd.merge_ordered(\n",
    "        df_temp_all_updated[[\"Sensor\", \"new_label\", \"Cluster\"]],\n",
    "        df_dni_all_updated[[\"Sensor\", \"new_label\", \"Cluster\"]],\n",
    "        on=\"Sensor\",\n",
    "    )\n",
    "    data_all.set_index(\"Sensor\", inplace=True)\n",
    "    return data_all\n",
    "\n",
    "\n",
    "df_all = merge_all_cluster_data(df_radiation_all_updated, df_temperature_all_updated)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6969aadc245e7fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.3: Correlation Analysis Between Temperature and Radiation Clusters\n",
    "*This section focuses on analyzing the correlation between temperature and radiation clusters.\n",
    "It includes two primary functions: `plot_cluster_correlation_heatmap` and `calculate_pearson_correlation`.\n",
    "The first function plots a heatmap to visualize the Pearson correlation between the clusters,\n",
    "enhancing understanding of their interrelationship.\n",
    "The second function calculates the actual Pearson correlation coefficient and its corresponding p-value,\n",
    "providing statistical evidence of the correlationâs significance.\n",
    "Together,\n",
    "these functions offer a comprehensive view of how closely related the temperature and radiation clusters are in the dataset.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22651a515fa5b364"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_cluster_correlation_heatmap(cluster_data):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of Pearson correlation between Temperature and Radiation clusters.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (pd.DataFrame): DataFrame containing temperature and radiation cluster data.\n",
    "    \"\"\"\n",
    "    cluster_data.rename(columns={'Cluster_x': 'Temperature Cluster', 'Cluster_y': 'Radiation Cluster'}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cluster_data[['Temperature Cluster', 'Radiation Cluster']].corr(),\n",
    "        annot=True, cmap=\"icefire\", vmin=-1, vmax=1, fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/correlation_heatmap_Clusters.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_pearson_correlation(cluster_data):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between Temperature and Radiation clusters.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (pd.DataFrame): DataFrame containing temperature and radiation cluster data.\n",
    "    \"\"\"\n",
    "    correlation, p_value = pearsonr(cluster_data[\"Temperature Cluster\"], cluster_data[\"Radiation Cluster\"])\n",
    "    print('Correlation:', correlation, 'P-value:', p_value)\n",
    "\n",
    "\n",
    "plot_cluster_correlation_heatmap(df_all)\n",
    "calculate_pearson_correlation(df_all)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53b9f0836b4341d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.4: Visualizing Mean Cluster Values\n",
    "*The focus here is on visualizing the mean values for each cluster.\n",
    "The code sorts clusters by mean values and utilizes seaborn to create descriptive box plots.\n",
    "This visual representation helps in understanding the distribution and central tendencies within each cluster.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55a0409feb7fbd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_mean_cluster_values(cluster_data, plot_title, y_label, mode):\n",
    "    \"\"\"\n",
    "    Plot box plots for cluster statistics.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (pd.DataFrame): DataFrame containing cluster statistics data.\n",
    "        plot_title (str): Title for the plot.\n",
    "        y_label (str): Label for the y-axis.\n",
    "        mode (str): Mode indicating the type of data (e.g., 'temperature' or 'radiation').\n",
    "    \"\"\"\n",
    "    sorted_clusters = cluster_data.median().sort_values().index\n",
    "    print(cluster_data.median().sort_values())\n",
    "\n",
    "    cluster_data_sorted = cluster_data[sorted_clusters]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=cluster_data_sorted, palette=\"colorblind\", linewidth=1, saturation=0.5, showfliers=True)\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/boxplot_{mode}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_mean_cluster_values(mean_all_cluster_temperature, plot_title=\"Box plots for Cluster Statistics Temperature\",\n",
    "                         y_label=\"Mean Temperature [Â°C]\", mode='temperature')\n",
    "plot_mean_cluster_values(mean_all_cluster_radiation, plot_title=\"Box plots for Cluster Statistics Radiation\",\n",
    "                         y_label=\"Mean Solar Voltage [V]\", mode='radiation')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc96bd22ceae9b11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Merge cluster data from temperature and DNI datasets*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da0713b10cac4c90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_updated_all = pd.merge_ordered(df_radiation_all_updated, df_temperature_all_updated,\n",
    "                                  on='Sensor')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312f72ef909ab32c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Part 9: Creating a Map with Folium\n",
    "---\n",
    "*This script uses Folium to create an interactive map displaying cluster data.\n",
    "It involves placing markers for each sensor based on latitude and longitude,\n",
    "with marker colors representing temperature and radiation clusters.\n",
    "The script also integrates a custom legend,\n",
    "enhancing the mapâs interpretability and providing a geographical context to the cluster analysis.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9d7fd6836933e6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a map centered around the mean latitude and longitude\n",
    "map_center = [survey_data[\"latitude\"].mean(), survey_data[\"longitude\"].mean()]\n",
    "my_map = folium.Map(location=map_center, zoom_start=8, max_zoom=15, min_zoom=8)\n",
    "\n",
    "df_updated_all = df_updated_all.dropna()\n",
    "temperature_colors = {\"cool\": 'blue', \"warm\": 'orange', \"hot\": 'red'}\n",
    "radiation_colors = {\"dark\": 'lightblue', \"medium dark\": 'blue', \"medium bright\": 'orange', \"bright\": 'red'}\n",
    "\n",
    "# Add markers to the map based on cluster information\n",
    "for idx, row in df_updated_all.iterrows():\n",
    "    sensor_id = row[\"Sensor\"]\n",
    "    temp_cluster = row[\"new_label_y\"]\n",
    "    rad_cluster = row[\"new_label_x\"]\n",
    "    temp_color = temperature_colors[temp_cluster]\n",
    "    rad_color = radiation_colors[rad_cluster]\n",
    "\n",
    "    # Get the latitude and longitude from the survey data\n",
    "    sensor_data = survey_data[survey_data[\"deviceId_boum\"].str.contains(sensor_id)]\n",
    "    if not sensor_data.empty:\n",
    "        lat, lon = sensor_data[[\"latitude\", \"longitude\"]].values[0]\n",
    "        folium.Marker(location=(lat, lon), radius=8, icon=folium.Icon(color=temp_color, icon=\"filled\"), fill=True,\n",
    "                      fill_opacity=0.7, tooltip=f\"Temperature: {temp_cluster.capitalize()}\", ).add_to(my_map)\n",
    "        folium.CircleMarker(location=(lat, lon), radius=8, color=rad_color, fill=True, fill_opacity=0.7,\n",
    "                            tooltip=f\"Radiation: {rad_cluster.capitalize()}\", ).add_to(my_map)\n",
    "\n",
    "# Define the template for the legend\n",
    "template = \"\"\"\n",
    "{% macro html(this, kwargs) %}\n",
    "<div style=\"position: fixed; \n",
    "     bottom: 2px; left: 6px; width: 180px; height: %s; \n",
    "     border:2px solid grey; z-index:9999; font-size:14px;\n",
    "     background:white; padding:5px;\">\n",
    "     <b>Temperature</b><br>\n",
    "     &nbsp; Cool &nbsp; <i class=\"fa fa-map-marker fa-2x\" style=\"color:blue\"></i><br>\n",
    "     &nbsp; Warm &nbsp; <i class=\"fa fa-map-marker fa-2x\" style=\"color:orange\"></i><br>\n",
    "     &nbsp; Hot &nbsp; <i class=\"fa fa-map-marker fa-2x\" style=\"color:red\"></i><br\n",
    "     <hr>\n",
    "    <b>Radiation</b> <br>\n",
    "     &nbsp; Low &nbsp; <i class=\"fa fa-circle fa-2x\" style=\"color:lightblue\"></i><br>\n",
    "     &nbsp; Medium Low &nbsp; <i class=\"fa fa-circle fa-2x\" style=\"color:blue\"></i><br>\n",
    "     &nbsp; Medium High &nbsp; <i class=\"fa fa-circle fa-2x\" style=\"color:orange\"></i><br>\n",
    "     &nbsp; High &nbsp; <i class=\"fa fa-circle fa-2x\" style=\"color:red\"></i><br>\n",
    "</div>\n",
    "{% endmacro %}\n",
    "\"\"\"\n",
    "macro = MacroElement()\n",
    "macro._template = Template(template)\n",
    "my_map.get_root().add_child(macro)\n",
    "my_map.save(\"../plots/map.html\")\n",
    "my_map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11fd57d0fc0b39dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
